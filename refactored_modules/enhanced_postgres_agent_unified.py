# refactored_modules/enhanced_postgres_agent_unified_fixed.py
# üîß FIXED: Added missing SQL extraction methods

import os
import time
import re
import json
import asyncio
import aiohttp
import psycopg2
from decimal import Decimal
from datetime import datetime, date
from typing import Dict, Any, Optional, List, Tuple, AsyncGenerator
import logging
from .intelligent_schema_discovery import EnhancedSchemaIntegration
# Import configs only
from .tenant_config import TenantConfigManager, TenantConfig

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnifiedEnhancedPostgresOllamaAgent:
    """ü§ñ FIXED: Enhanced PostgreSQL Agent with ALL required methods"""
    
    def __init__(self):
        """üèóÔ∏è Initialize unified agent"""
        
        # üîß Configuration
        self.config_manager = TenantConfigManager()
        self.tenant_configs = self.config_manager.tenant_configs
        
        # üåê Ollama Configuration
        self.ollama_base_url = os.getenv('OLLAMA_BASE_URL', 'http://52.74.36.160:12434')
        self.request_timeout = int(os.getenv('AI_REQUEST_TIMEOUT', '90'))
        self.max_retries = int(os.getenv('AI_MAX_RETRIES', '3'))
        
        # üÜï AI Response Configuration
        self.enable_ai_responses = os.getenv('ENABLE_AI_RESPONSES', 'true').lower() == 'true'
        self.ai_response_temperature = float(os.getenv('AI_RESPONSE_TEMPERATURE', '0.3'))
        self.fallback_to_hardcode = os.getenv('FALLBACK_TO_HARDCODE', 'true').lower() == 'true'
        
        # üìä Performance tracking
        self.stats = {
            'total_queries': 0,
            'sql_queries': 0,
            'conversational_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'ai_responses_used': 0,
            'hardcode_responses_used': 0,
            'avg_response_time': 0.0
        }
        
        # üß† Schema cache
        self.schema_cache = {}
        self.cache_ttl = 3600  # 1 hour
        
        # üéØ Intent detection keywords
        self.sql_indicators = {
            'identification': ['‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà', '‡πÉ‡∏Ñ‡∏£‡πÄ‡∏õ‡πá‡∏ô', '‡πÉ‡∏Ñ‡∏£‡∏ó‡∏≥', 'who is', 'who are', 'who works'],
            'listing': ['‡πÉ‡∏Ñ‡∏£‡∏ö‡πâ‡∏≤‡∏á', '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠', 'list', '‡πÅ‡∏™‡∏î‡∏á', 'show me', 'display'],
            'counting': ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô', 'how many', 'count', '‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£', '‡∏°‡∏µ‡∏Å‡∏µ‡πà'],
            'searching': ['‡∏´‡∏≤', '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤', 'find', 'search', '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á', 'position'],
            'filtering': ['‡πÅ‡∏ú‡∏ô‡∏Å', 'department', '‡∏ù‡πà‡∏≤‡∏¢', '‡∏á‡∏≤‡∏ô', '‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ', 'project'],
            'relationships': ['‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô', 'assigned', 'working on', 'responsible']
        }
        
        self.conversational_indicators = {
            'greetings': ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', 'hello', 'hi', '‡∏ä‡πà‡∏ß‡∏¢', 'help'],
            'general_info': ['‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏Ñ‡∏£', '‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö', 'about', 'what are you'],
            'capabilities': ['‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ', '‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£', 'what can you do']
        }
        
        try:
            from .intelligent_schema_discovery import EnhancedSchemaIntegration
            self.schema_integration = EnhancedSchemaIntegration(
                database_handler=self,  # ‡∏™‡πà‡∏á self ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ method _get_database_connection
                tenant_configs=self.tenant_configs
            )
            self.intelligent_schema_available = True
            logger.info("üß† Intelligent Schema Discovery integrated successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Intelligent Schema Discovery: {e}")
            self.schema_integration = None
            self.intelligent_schema_available = False
            logger.warning("‚ö†Ô∏è Falling back to basic schema discovery")
        logger.info("ü§ñ FIXED Unified Enhanced PostgreSQL Agent initialized")
        logger.info(f"üåê Ollama: {self.ollama_base_url}")
        logger.info(f"üé® AI Responses: {'Enabled' if self.enable_ai_responses else 'Disabled'}")
        logger.info(f"üè¢ Tenants: {list(self.tenant_configs.keys())}")
    
    # ========================================================================
    # üéØ MAIN ENTRY POINT
    # ========================================================================
    
    async def process_enhanced_question(self, question: str, tenant_id: str) -> Dict[str, Any]:
        """üéØ MAIN: Process questions with unified logic + AI responses"""
        
        if tenant_id not in self.tenant_configs:
            return self._create_error_response("Unknown tenant", tenant_id)
        
        start_time = datetime.now()
        self.stats['total_queries'] += 1
        
        try:
            logger.info(f"üéØ Processing question for {tenant_id}: {question[:50]}...")
            
            # üîç Enhanced Intent Detection
            intent_result = self._detect_intent_unified(question)
            logger.info(f"üéØ Intent: {intent_result['intent']} (confidence: {intent_result['confidence']:.2f})")
            
            # üîÄ Route based on intent
            if intent_result['intent'] == 'conversational' and intent_result['confidence'] >= 0.6:
                result = await self._process_conversational_unified(question, tenant_id, intent_result)
            elif intent_result['intent'] == 'sql_query' and intent_result['confidence'] >= 0.5:
                result = await self._process_sql_unified_with_ai_response(question, tenant_id, intent_result)
            else:
                # Hybrid approach for ambiguous cases
                result = await self._process_hybrid_unified(question, tenant_id, intent_result)
            
            # üìä Update statistics
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(tenant_id, True, processing_time)
            
            result['processing_time_seconds'] = processing_time
            result['unified_agent_version'] = 'v3.1_fixed'
            result['intent_detection'] = intent_result
            
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(tenant_id, False, processing_time)
            logger.error(f"‚ùå Processing failed for {tenant_id}: {e}")
            return self._create_error_response(str(e), tenant_id)
    
    # ========================================================================
    # üéØ SQL PROCESSING WITH AI RESPONSE
    # ========================================================================
    
    async def _process_sql_unified_with_ai_response(self, question: str, tenant_id: str, intent_result: Dict) -> Dict[str, Any]:
        """üéØ Fixed SQL processing with intelligent schema"""
        
        self.stats['sql_queries'] += 1
        
        try:
            # üÜï ‡πÉ‡∏ä‡πâ Intelligent Schema ‡∏´‡∏≤‡∏Å‡∏°‡∏µ
            if self.intelligent_schema_available and self.schema_integration:
                try:
                    sql_prompt = await self.schema_integration.generate_intelligent_sql_prompt(question, tenant_id)
                    logger.info("‚úÖ Using Intelligent Schema Discovery")
                except Exception as e:
                    logger.warning(f"üîÑ Intelligent schema failed: {e}, falling back to basic")
                    # Fallback to basic schema
                    schema_info = await self._get_schema_unified(tenant_id)
                    sql_prompt = self._generate_sql_prompt_unified(question, tenant_id, schema_info, intent_result)
            else:
                # ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
                schema_info = await self._get_schema_unified(tenant_id)
                sql_prompt = self._generate_sql_prompt_unified(question, tenant_id, schema_info, intent_result)
            
            # ü§ñ Call AI service for SQL generation (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
            ai_response = await self._call_ollama_unified(tenant_id, sql_prompt)
            
            # üîç Extract and validate SQL (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
            sql_result = self._extract_sql_unified(ai_response, question)
            
            if not sql_result['success']:
                raise ValueError(f"SQL extraction failed: {sql_result['error']}")
            
            sql_query = sql_result['sql']
            
            # üóÑÔ∏è Execute SQL (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
            db_results = await self._execute_sql_unified(sql_query, tenant_id)
            
            # üÜï Generate AI response (Fixed parameters)
            if self.enable_ai_responses and db_results:
                try:
                    formatted_answer = await self._generate_ai_response_from_data(
                        question, db_results, tenant_id, sql_query, schema_info=None  # üîß Fixed
                    )
                    response_method = 'ai_generated'
                    self.stats['ai_responses_used'] += 1
                    
                except Exception as ai_error:
                    logger.warning(f"üîÑ AI response generation failed: {ai_error}, falling back to hardcode")
                    if self.fallback_to_hardcode:
                        formatted_answer = self._format_response_hardcode(
                            db_results, question, tenant_id, sql_query, schema_info=None  # üîß Fixed
                        )
                        response_method = 'hardcode_fallback'
                        self.stats['hardcode_responses_used'] += 1
                    else:
                        raise ai_error
            else:
                # Use hardcode formatting (Fixed parameters)
                formatted_answer = self._format_response_hardcode(
                    db_results, question, tenant_id, sql_query, schema_info=None  # üîß Fixed
                )
                response_method = 'hardcode_default'
                self.stats['hardcode_responses_used'] += 1
            
            return {
                "answer": formatted_answer,
                "success": True,
                "data_source_used": f"fixed_intelligent_schema_{self.tenant_configs[tenant_id].model_name}",
                "sql_query": sql_query,
                "db_results_count": len(db_results) if db_results else 0,
                "tenant_id": tenant_id,
                "system_used": f"fixed_intelligent_schema_with_{response_method}",
                "sql_extraction_method": sql_result['method'],
                "sql_confidence": sql_result['confidence'],
                "response_generation_method": response_method,
                "intelligent_schema_used": self.intelligent_schema_available
            }
            
        except Exception as e:
            logger.error(f"‚ùå SQL processing failed: {e}")
            return self._create_sql_error_response(question, tenant_id, str(e))
    
    # üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏° method ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö management
    async def get_intelligent_schema_stats(self) -> Dict[str, Any]:
        """üìä ‡∏î‡∏π‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö Intelligent Schema Discovery"""
        return self.schema_integration.get_system_statistics()
    
    def clear_schema_cache(self, tenant_id: Optional[str] = None):
        """üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á cache ‡∏Ç‡∏≠‡∏á schema discovery"""
        self.schema_integration.schema_discovery.clear_cache(tenant_id)
    async def get_intelligent_schema_stats(self) -> Dict[str, Any]:
        """üìä ‡∏î‡∏π‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö Intelligent Schema Discovery"""
        
        if self.intelligent_schema_available and self.schema_integration:
            try:
                cache_stats = self.schema_integration.schema_discovery.get_cache_statistics()
                return {
                    'intelligent_schema_system': 'active',
                    'cache_statistics': cache_stats,
                    'features': [
                        'contextual_schema_discovery',
                        'intelligent_prompt_building',
                        'question_analysis',
                        'relevant_data_extraction',
                        'smart_caching'
                    ]
                }
            except Exception as e:
                return {'error': f'Failed to get stats: {str(e)}'}
        else:
            return {'intelligent_schema_system': 'not_available'}

    def clear_schema_cache(self, tenant_id: Optional[str] = None):
        """üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á cache ‡∏Ç‡∏≠‡∏á schema discovery"""
        
        if self.intelligent_schema_available and self.schema_integration:
            try:
                self.schema_integration.schema_discovery.clear_cache(tenant_id)
                logger.info(f"üóëÔ∏è Schema cache cleared for {tenant_id if tenant_id else 'all tenants'}")
            except Exception as e:
                logger.error(f"‚ùå Failed to clear cache: {e}")
        else:
            logger.warning("‚ö†Ô∏è Intelligent Schema Discovery not available")
    # ========================================================================
    # üîç SQL EXTRACTION - FIXED with ALL methods
    # ========================================================================
    
    def _extract_sql_unified(self, ai_response: str, question: str) -> Dict[str, Any]:
        """üîç FIXED: Extract SQL with ALL required methods"""
        
        logger.info(f"üîç Extracting SQL from response (length: {len(ai_response)})")
        
        extraction_result = {
            'success': False,
            'sql': None,
            'method': None,
            'confidence': 0.0,
            'error': None
        }
        
        # üîß FIXED: All extraction methods are now included
        extraction_methods = [
            ('sql_code_block_complete', self._extract_complete_sql_block),
            ('multiline_select_complete', self._extract_multiline_select),  # ‚úÖ FIXED
            ('single_line_complete', self._extract_single_line_select),     # ‚úÖ FIXED
            ('intelligent_fallback', self._create_intelligent_fallback)
        ]
        
        for method_name, method_func in extraction_methods:
            try:
                sql = method_func(ai_response, question)
                
                if sql and self._validate_complete_sql(sql):
                    confidence = self._calculate_sql_confidence(sql, question, method_name)
                    
                    if confidence > extraction_result['confidence']:
                        extraction_result.update({
                            'success': True,
                            'sql': sql,
                            'method': method_name,
                            'confidence': confidence
                        })
                        
                        logger.info(f"‚úÖ Valid SQL found: {method_name} (confidence: {confidence:.2f})")
                        
                        if confidence >= 0.8:
                            break
                            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Method {method_name} failed: {e}")
                continue
        
        if not extraction_result['success']:
            extraction_result['error'] = "No valid SQL could be extracted or generated"
            logger.error(f"‚ùå All SQL extraction methods failed for: {question[:50]}...")
        
        return extraction_result
    
    def _extract_complete_sql_block(self, response: str, question: str) -> Optional[str]:
        """üîç Extract complete SQL from code blocks"""
        
        patterns = [
            r'```sql\s*(SELECT.*?)\s*```',
            r'```sql\s*(.*?SELECT.*?)\s*```',
            r'```\s*(SELECT.*?FROM.*?)\s*```'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                sql = self._clean_sql_thoroughly(match.group(1))
                if self._has_required_clauses(sql):
                    return sql
        
        return None
    
    def _extract_multiline_select(self, response: str, question: str) -> Optional[str]:
        """üîç FIXED: Extract multiline SELECT statements"""
        
        # Look for multiline SELECT patterns
        patterns = [
            # Standard multiline with proper formatting
            r'SELECT\s+.*?FROM\s+.*?(?:WHERE\s+.*?)?(?:ORDER\s+BY\s+.*?)?(?:LIMIT\s+\d+)?[;\s]*',
            
            # With JOIN
            r'SELECT\s+.*?FROM\s+.*?JOIN\s+.*?(?:WHERE\s+.*?)?(?:ORDER\s+BY\s+.*?)?(?:LIMIT\s+\d+)?[;\s]*',
            
            # With aliases
            r'SELECT\s+.*?FROM\s+\w+\s+\w+.*?(?:WHERE\s+.*?)?(?:ORDER\s+BY\s+.*?)?(?:LIMIT\s+\d+)?[;\s]*'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, response, re.DOTALL | re.IGNORECASE)
            for match in matches:
                sql = self._clean_sql_thoroughly(match.group(0))
                if self._has_required_clauses(sql) and len(sql) > 30:
                    return sql
        
        return None
    
    def _extract_single_line_select(self, response: str, question: str) -> Optional[str]:
        """üîç FIXED: Extract single line SELECT statements"""
        
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # Skip code block markers
            if line.startswith('```') or not line:
                continue
            
            # Look for SELECT statements
            if re.match(r'^SELECT\s+', line, re.IGNORECASE):
                sql = self._clean_sql_thoroughly(line)
                
                # Basic validation
                if ('FROM' in sql.upper() and 
                    len(sql) > 20 and 
                    self._has_required_clauses(sql)):
                    return sql
        
        return None
    
    def _clean_sql_thoroughly(self, sql: str) -> str:
        """üßπ Thorough SQL cleaning"""
        
        if not sql:
            return ""
        
        # Remove artifacts
        sql = sql.strip().rstrip(';').strip()
        sql = re.sub(r'^```sql\s*', '', sql, flags=re.IGNORECASE)
        sql = re.sub(r'```\s*$', '', sql)
        
        # Normalize whitespace
        sql = re.sub(r'\s+', ' ', sql)
        
        # Remove duplicates
        sql = re.sub(r'\bSELECT\s+SELECT\b', 'SELECT', sql, flags=re.IGNORECASE)
        sql = re.sub(r'\bFROM\s+FROM\b', 'FROM', sql, flags=re.IGNORECASE)
        
        # Proper keyword casing
        keywords = ['SELECT', 'FROM', 'WHERE', 'JOIN', 'LEFT JOIN', 'INNER JOIN', 
                   'ORDER BY', 'GROUP BY', 'LIMIT', 'AS', 'ON', 'AND', 'OR']
        
        for keyword in keywords:
            pattern = r'\b' + keyword.replace(' ', r'\s+') + r'\b'
            sql = re.sub(pattern, keyword, sql, flags=re.IGNORECASE)
        
        return sql.strip()
    
    def _has_required_clauses(self, sql: str) -> bool:
        """üîç Check if SQL has required clauses"""
        
        if not sql or len(sql) < 15:
            return False
        
        sql_upper = sql.upper()
        
        # Must have SELECT and FROM
        if not sql_upper.startswith('SELECT'):
            return False
        
        if 'FROM' not in sql_upper:
            return False
        
        # Check for undefined aliases
        if self._has_undefined_aliases(sql):
            return False
        
        return True
    
    def _has_undefined_aliases(self, sql: str) -> bool:
        """üîç Check for undefined table aliases"""
        
        # Find alias usage (e.g., e.name, p.id)
        alias_usage = re.findall(r'\b([a-zA-Z])\.\w+', sql)
        
        if not alias_usage:
            return False
        
        # Check if aliases are defined
        for alias in set(alias_usage):
            alias_patterns = [
                rf'\b\w+\s+{alias}\b',
                rf'\b\w+\s+AS\s+{alias}\b'
            ]
            
            alias_defined = any(
                re.search(pattern, sql, re.IGNORECASE) 
                for pattern in alias_patterns
            )
            
            if not alias_defined:
                logger.warning(f"‚ö†Ô∏è Undefined alias '{alias}' in SQL")
                return True
        
        return False
    
    def _validate_complete_sql(self, sql: str) -> bool:
        """üîç Validate that SQL is complete and safe"""
        
        if not sql or len(sql) < 15:
            return False
        
        sql_upper = sql.upper()
        
        # Security checks
        dangerous_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE']
        if any(keyword in sql_upper for keyword in dangerous_keywords):
            logger.warning(f"üö® Dangerous SQL detected")
            return False
        
        # Structure checks
        if not sql_upper.startswith('SELECT'):
            return False
        
        if 'FROM' not in sql_upper:
            return False
        
        # Alias consistency
        if self._has_undefined_aliases(sql):
            return False
        
        return True
    
    def _calculate_sql_confidence(self, sql: str, question: str, method: str) -> float:
        """üîç Calculate confidence score for SQL"""
        
        confidence = 0.0
        
        # Base confidence by method
        method_confidence = {
            'sql_code_block_complete': 0.9,
            'multiline_select_complete': 0.8,
            'single_line_complete': 0.7,
            'intelligent_fallback': 0.6
        }
        confidence += method_confidence.get(method, 0.3)
        
        # Boost for relevance
        sql_lower = sql.lower()
        question_lower = question.lower()
        
        relevance_boost = 0.0
        if '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á' in question_lower and 'position' in sql_lower:
            relevance_boost += 0.1
        if '‡πÅ‡∏ú‡∏ô‡∏Å' in question_lower and 'department' in sql_lower:
            relevance_boost += 0.1
        if any(word in question_lower for word in ['‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ']) and 'join' in sql_lower:
            relevance_boost += 0.1
        if any(word in question_lower for word in ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô']) and 'count' in sql_lower:
            relevance_boost += 0.1
        
        confidence += relevance_boost
        
        # Quality indicators
        if 'LIMIT' in sql.upper():
            confidence += 0.05
        if len(sql) > 30 and len(sql) < 300:
            confidence += 0.05
        
        return min(confidence, 1.0)
    
    def _create_intelligent_fallback(self, response: str, question: str) -> Optional[str]:
        """üîÑ Create intelligent SQL fallback"""
        
        question_lower = question.lower()
        
        # Employee-Project relationship
        if any(word in question_lower for word in ['‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô', '‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ', 'assigned']):
            return """SELECT 
                e.name as employee_name,
                p.name as project_name,
                ep.role,
                ep.allocation
            FROM employees e
            JOIN employee_projects ep ON e.id = ep.employee_id
            JOIN projects p ON ep.project_id = p.id
            ORDER BY e.name
            LIMIT 20"""
        
        # Position search
        elif '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á' in question_lower or 'position' in question_lower:
            position = self._extract_position_keyword(question)
            return f"""SELECT name, position, department, salary
            FROM employees
            WHERE position ILIKE '%{position}%'
            ORDER BY name
            LIMIT 20"""
        
        # Department counting
        elif '‡πÅ‡∏ú‡∏ô‡∏Å' in question_lower and any(word in question_lower for word in ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô']):
            return """SELECT department, COUNT(*) as employee_count, AVG(salary) as avg_salary
            FROM employees
            GROUP BY department
            ORDER BY employee_count DESC"""
        
        # General employee listing
        else:
            return """SELECT name, position, department, salary
            FROM employees
            ORDER BY name
            LIMIT 20"""
    
    def _extract_position_keyword(self, question: str) -> str:
        """Extract position keyword from question"""
        
        question_lower = question.lower()
        
        position_keywords = ['frontend', 'backend', 'fullstack', 'developer', 'designer', 'manager', 'qa', 'devops']
        
        for keyword in position_keywords:
            if keyword in question_lower:
                return keyword
        
        # Try to extract after "‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á"
        match = re.search(r'‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á\s*(\w+)', question_lower)
        if match:
            return match.group(1)
        
        return 'developer'  # default
    
    # ========================================================================
    # üîß ADD ALL MISSING METHODS
    # ========================================================================
    
    def _detect_intent_unified(self, question: str) -> Dict[str, Any]:
        """üéØ UNIFIED: Enhanced intent detection"""
        
        question_lower = question.lower()
        
        # Calculate SQL indicators score
        sql_score = 0
        sql_reasons = []
        
        for category, keywords in self.sql_indicators.items():
            matches = [word for word in keywords if word in question_lower]
            if matches:
                weight = 3 if category in ['identification', 'counting', 'relationships'] else 2
                sql_score += len(matches) * weight
                sql_reasons.append(f"{category}: {matches}")
        
        # Calculate conversational indicators score
        conv_score = 0
        conv_reasons = []
        
        for category, keywords in self.conversational_indicators.items():
            matches = [word for word in keywords if word in question_lower]
            if matches:
                conv_score += len(matches) * 3
                conv_reasons.append(f"{category}: {matches}")
        
        # Special pattern detection
        if self._has_sql_patterns(question_lower):
            sql_score += 5
            sql_reasons.append("sql_pattern_detected")
        
        if self._has_conversational_patterns(question_lower):
            conv_score += 5
            conv_reasons.append("conversational_pattern_detected")
        
        # Determine intent
        total_score = sql_score + conv_score
        
        if total_score == 0:
            return {'intent': 'unknown', 'confidence': 0.0, 'reasons': ['no_clear_indicators']}
        
        if conv_score > sql_score:
            return {
                'intent': 'conversational',
                'confidence': conv_score / total_score,
                'sql_score': sql_score,
                'conv_score': conv_score,
                'reasons': conv_reasons
            }
        else:
            return {
                'intent': 'sql_query',
                'confidence': sql_score / total_score,
                'sql_score': sql_score,
                'conv_score': conv_score,
                'reasons': sql_reasons
            }
    
    def _has_sql_patterns(self, question_lower: str) -> bool:
        """Check for SQL-specific patterns"""
        sql_patterns = [
            r'‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà.*‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á',
            r'‡∏°‡∏µ.*‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô.*‡πÅ‡∏ú‡∏ô‡∏Å',
            r'‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠.*‡∏ó‡∏µ‡πà',
            r'‡πÅ‡∏™‡∏î‡∏á.*‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•',
            r'.*‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö.*‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ',
            r'who.*in.*position',
            r'how many.*in'
        ]
        return any(re.search(pattern, question_lower) for pattern in sql_patterns)
    
    def _has_conversational_patterns(self, question_lower: str) -> bool:
        """Check for conversational patterns"""
        conv_patterns = [
            r'‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ.*‡∏Ñ‡∏£‡∏±‡∏ö',
            r'‡∏Ñ‡∏∏‡∏ì.*‡∏Ñ‡∏∑‡∏≠.*‡πÉ‡∏Ñ‡∏£',
            r'‡∏ä‡πà‡∏ß‡∏¢.*‡∏≠‡∏∞‡πÑ‡∏£.*‡πÑ‡∏î‡πâ',
            r'hello.*there',
            r'what.*are.*you'
        ]
        return any(re.search(pattern, question_lower) for pattern in conv_patterns)
    
    def _generate_sql_prompt_unified(self, question: str, tenant_id: str, 
                                   schema_info: Dict, intent_result: Dict) -> str:
        """üéØ UNIFIED: Generate SQL prompt"""
        
        config = self.tenant_configs[tenant_id]
        business_context = self._get_business_context_unified(tenant_id)
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ PostgreSQL Expert ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {config.name}

{business_context}

üìä ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á):
‚Ä¢ employees: id, name, department, position, salary, hire_date, email
‚Ä¢ projects: id, name, client, budget, status, start_date, end_date, tech_stack
‚Ä¢ employee_projects: employee_id, project_id, role, allocation

üîó ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
‚Ä¢ employee_projects.employee_id ‚Üí employees.id
‚Ä¢ employee_projects.project_id ‚Üí projects.id

üîß ‡∏Å‡∏é SQL ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏´‡πâ‡∏≤‡∏°‡∏ú‡∏¥‡∏î):
1. SQL ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ FROM clause ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
2. ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ alias (e, p, ep) ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô FROM/JOIN
3. ‡πÉ‡∏ä‡πâ JOIN ‡πÅ‡∏ó‡∏ô WHERE ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏≤‡∏£‡∏≤‡∏á
4. ‡πÉ‡∏ä‡πâ ILIKE '%keyword%' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
5. ‡πÉ‡∏ä‡πâ LIMIT 20 ‡πÄ‡∏™‡∏°‡∏≠
6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö syntax ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô response

Intent: {intent_result['intent']} (confidence: {intent_result['confidence']:.2f})
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏™‡∏£‡πâ‡∏≤‡∏á PostgreSQL query ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ:"""
        
        return prompt
    
    # Add all other missing methods here...
    # (For brevity, I'll include the essential ones)
    
    async def _call_ollama_unified(self, tenant_id: str, prompt: str, 
                                 temperature: float = 0.1) -> str:
        """ü§ñ UNIFIED: AI API call"""
        
        config = self.tenant_configs[tenant_id]
        
        payload = {
            "model": config.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": 1500,
                "top_k": 20,
                "top_p": 0.8,
                "repeat_penalty": 1.0,
                "num_ctx": 4096
            }
        }
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"ü§ñ AI API call attempt {attempt + 1} for {tenant_id}")
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.ollama_base_url}/api/generate",
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=self.request_timeout)
                    ) as response:
                        
                        if response.status == 200:
                            result = await response.json()
                            response_text = result.get('response', '').strip()
                            
                            if response_text:
                                logger.info(f"‚úÖ AI API call successful for {tenant_id}")
                                return response_text
                            else:
                                raise ValueError("Empty response from AI")
                        else:
                            raise aiohttp.ClientResponseError(
                                request_info=response.request_info,
                                history=response.history,
                                status=response.status
                            )
                            
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ AI API timeout attempt {attempt + 1}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    
            except Exception as e:
                logger.warning(f"üîÑ AI API error attempt {attempt + 1}: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        
        raise Exception(f"All {self.max_retries} AI API attempts failed")
    
    # ========================================================================
    # üóÑÔ∏è DATABASE OPERATIONS
    # ========================================================================
    
    def _get_database_connection(self, tenant_id: str) -> psycopg2.extensions.connection:
        """üîå Get database connection"""
        
        config = self.tenant_configs[tenant_id]
        
        try:
            conn = psycopg2.connect(
                host=config.db_host,
                port=config.db_port,
                database=config.db_name,
                user=config.db_user,
                password=config.db_password,
                connect_timeout=10
            )
            conn.set_session(autocommit=True)
            return conn
            
        except Exception as e:
            logger.error(f"‚ùå Database connection failed for {tenant_id}: {e}")
            raise
    
    async def _execute_sql_unified(self, sql_query: str, tenant_id: str) -> List[Dict[str, Any]]:
        """üóÑÔ∏è UNIFIED: Execute SQL query"""
        
        try:
            logger.info(f"üóÑÔ∏è Executing SQL for {tenant_id}: {sql_query[:100]}...")
            
            conn = self._get_database_connection(tenant_id)
            cursor = conn.cursor()
            
            cursor.execute(sql_query)
            
            # Get results
            columns = [desc[0] for desc in cursor.description] if cursor.description else []
            rows = cursor.fetchall()
            
            # Process results
            results = []
            for row in rows:
                row_dict = dict(zip(columns, row))
                processed_row = self._process_row_data(row_dict)
                results.append(processed_row)
            
            cursor.close()
            conn.close()
            
            logger.info(f"‚úÖ SQL executed successfully: {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå SQL execution failed: {e}")
            logger.error(f"‚ùå Failed SQL: {sql_query}")
            return []
    
    def _process_row_data(self, row_dict: Dict[str, Any]) -> Dict[str, Any]:
        """üîß Process row data"""
        
        processed_row = {}
        
        for key, value in row_dict.items():
            if isinstance(value, Decimal):
                processed_row[key] = float(value)
            elif isinstance(value, (date, datetime)):
                processed_row[key] = value.isoformat()
            elif value is None:
                processed_row[key] = None
            elif isinstance(value, str):
                processed_row[key] = value.strip()
            else:
                processed_row[key] = value
        
        return processed_row
    
    # ========================================================================
    # üîç SCHEMA DISCOVERY
    # ========================================================================
    
    async def _get_schema_unified(self, tenant_id: str) -> Dict[str, Any]:
        """üîç UNIFIED: Get schema info with caching"""
        
        cache_key = f"{tenant_id}_schema"
        
        # Check cache
        if self._is_schema_cache_valid(cache_key):
            logger.info(f"üìä Using cached schema for {tenant_id}")
            return self.schema_cache[cache_key]['data']
        
        try:
            logger.info(f"üîç Discovering schema for {tenant_id}")
            schema_info = await self._discover_schema(tenant_id)
            
            # Cache results
            self.schema_cache[cache_key] = {
                'data': schema_info,
                'timestamp': time.time()
            }
            
            return schema_info
            
        except Exception as e:
            logger.error(f"‚ùå Schema discovery failed: {e}")
            return self._get_fallback_schema()
    
    def _is_schema_cache_valid(self, cache_key: str) -> bool:
        """Check cache validity"""
        if cache_key not in self.schema_cache:
            return False
        
        cache_age = time.time() - self.schema_cache[cache_key]['timestamp']
        return cache_age < self.cache_ttl
    
    async def _discover_schema(self, tenant_id: str) -> Dict[str, Any]:
        """Discover database schema"""
        
        try:
            conn = self._get_database_connection(tenant_id)
            cursor = conn.cursor()
            
            schema_info = {
                'tables': {},
                'sample_data': {},
                'discovered_at': datetime.now().isoformat()
            }
            
            # Get table structure
            cursor.execute("""
                SELECT table_name, column_name, data_type, is_nullable
                FROM information_schema.columns 
                WHERE table_schema = 'public' 
                AND table_name IN ('employees', 'projects', 'employee_projects')
                ORDER BY table_name, ordinal_position
            """)
            
            for row in cursor.fetchall():
                table_name, column_name, data_type, is_nullable = row
                
                if table_name not in schema_info['tables']:
                    schema_info['tables'][table_name] = {'columns': []}
                
                schema_info['tables'][table_name]['columns'].append({
                    'name': column_name,
                    'type': data_type,
                    'nullable': is_nullable == 'YES'
                })
            
            cursor.close()
            conn.close()
            return schema_info
            
        except Exception as e:
            logger.error(f"Schema discovery error: {e}")
            raise
    
    def _get_fallback_schema(self) -> Dict[str, Any]:
        """Fallback schema"""
        return {
            'tables': {
                'employees': {
                    'columns': [
                        {'name': 'id', 'type': 'integer', 'nullable': False},
                        {'name': 'name', 'type': 'character varying', 'nullable': False},
                        {'name': 'department', 'type': 'character varying', 'nullable': False},
                        {'name': 'position', 'type': 'character varying', 'nullable': False},
                        {'name': 'salary', 'type': 'numeric', 'nullable': False}
                    ]
                }
            },
            'sample_data': {},
            'discovered_at': datetime.now().isoformat(),
            'fallback': True
        }
    
    # ========================================================================
    # ü§ñ AI RESPONSE GENERATION
    # ========================================================================
    
    async def _generate_ai_response_from_data(self, question: str, db_results: List[Dict], 
                                            tenant_id: str, sql_query: str, 
                                            enable_streaming: bool = True) -> str:
        """ü§ñ Generate AI response with optional streaming"""
        
        config = self.tenant_configs[tenant_id]
        business_context = self._get_business_context_unified(tenant_id)
        business_emoji = self._get_business_emoji(tenant_id)
        
        # Prepare data summary for AI
        data_summary = self._prepare_data_summary_for_ai(db_results, tenant_id)
        
        # Create AI prompt for response generation
        response_prompt = self._create_ai_response_prompt(
            question, data_summary, tenant_id, business_context, business_emoji, sql_query
        )
        
        logger.info(f"ü§ñ Generating AI response for {tenant_id} with {len(db_results)} results")
        
        if enable_streaming:
            # üÜï Streaming response generation
            return await self._call_ollama_streaming(tenant_id, response_prompt)
        else:
            # Original non-streaming
            ai_response = await self._call_ollama_unified(
                tenant_id, response_prompt, temperature=self.ai_response_temperature
            )
            return self._post_process_ai_response(ai_response, tenant_id, len(db_results))
        
    async def _call_ollama_streaming(self, tenant_id: str, prompt: str, 
                                temperature: float = 0.3) -> AsyncGenerator[Dict[str, Any], None]:
        """üåä Call Ollama with streaming for response generation"""
        
        config = self.tenant_configs[tenant_id]
        
        payload = {
            "model": config.model_name,
            "prompt": prompt,
            "stream": True,  # ‚Üê Streaming enabled
            "options": {
                "temperature": temperature,
                "num_predict": 1500,
                "top_k": 20,
                "top_p": 0.8,
                "repeat_penalty": 1.0,
                "num_ctx": 4096
            }
        }
        
        try:
            logger.info(f"üåä Starting streaming AI call for {tenant_id}")
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_base_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.request_timeout)
                ) as response:
                    
                    if response.status == 200:
                        full_response = ""
                        
                        async for line in response.content:
                            if line:
                                try:
                                    line_str = line.decode('utf-8').strip()
                                    if line_str:
                                        chunk_data = json.loads(line_str)
                                        chunk_text = chunk_data.get('response', '')
                                        
                                        if chunk_text:
                                            full_response += chunk_text
                                            
                                            # Yield each chunk to user
                                            yield {
                                                "type": "response_chunk",
                                                "content": chunk_text,
                                                "tenant_id": tenant_id,
                                                "accumulated": full_response
                                            }
                                        
                                        # Check if complete
                                        if chunk_data.get('done', False):
                                            # Send completion signal
                                            yield {
                                                "type": "response_complete",
                                                "content": "",
                                                "final_response": self._post_process_ai_response(
                                                    full_response, tenant_id, 0
                                                ),
                                                "tenant_id": tenant_id
                                            }
                                            break
                                            
                                except json.JSONDecodeError:
                                    continue
                    else:
                        yield {
                            "type": "error",
                            "message": f"Ollama API error: HTTP {response.status}",
                            "tenant_id": tenant_id
                        }
                        
        except Exception as e:
            logger.error(f"‚ùå Streaming AI call failed for {tenant_id}: {e}")
            yield {
                "type": "error", 
                "message": f"AI streaming error: {str(e)}",
                "tenant_id": tenant_id
            }

    async def _process_sql_unified_with_streaming_response(self, question: str, tenant_id: str, 
                                                        intent_result: Dict) -> AsyncGenerator[Dict[str, Any], None]:
        """üéØ SQL processing with streaming response generation"""
        
        self.stats['sql_queries'] += 1
        
        try:
            # Step 1-5: Same as before (Non-streaming SQL generation)
            schema_info = await self._get_schema_unified(tenant_id)
            sql_prompt = self._generate_sql_prompt_unified(question, tenant_id, schema_info, intent_result)
            
            # SQL Generation: Non-streaming (for accuracy)
            ai_response = await self._call_ollama_unified(tenant_id, sql_prompt)
            sql_result = self._extract_sql_unified(ai_response, question)
            
            if not sql_result['success']:
                yield {
                    "type": "error",
                    "message": f"SQL generation failed: {sql_result['error']}",
                    "tenant_id": tenant_id
                }
                return
            
            sql_query = sql_result['sql']
            
            # Execute SQL
            db_results = await self._execute_sql_unified(sql_query, tenant_id)
            
            # Send metadata first
            yield {
                "type": "metadata",
                "sql_query": sql_query,
                "db_results_count": len(db_results),
                "tenant_id": tenant_id,
                "status": "generating_response"
            }
            
            # Step 6: Response Generation with Streaming
            if self.enable_ai_responses and db_results:
                async for chunk in self._generate_ai_response_streaming(
                    question, db_results, tenant_id, sql_query
                ):
                    yield chunk
            else:
                # Fallback to hardcode
                hardcode_response = self._format_response_hardcode(
                    db_results, question, tenant_id, sql_query
                )
                yield {
                    "type": "response_complete",
                    "final_response": hardcode_response,
                    "tenant_id": tenant_id,
                    "method": "hardcode_fallback"
                }
                
        except Exception as e:
            yield {
                "type": "error",
                "message": f"Processing failed: {str(e)}",
                "tenant_id": tenant_id
            }

    async def _generate_ai_response_streaming(self, question: str, db_results: List[Dict], 
                                            tenant_id: str, sql_query: str) -> AsyncGenerator[Dict[str, Any], None]:
        """üåä Generate streaming AI response from database results"""
        
        try:
            # Prepare prompt (same as before)
            data_summary = self._prepare_data_summary_for_ai(db_results, tenant_id)
            response_prompt = self._create_ai_response_prompt(
                question, data_summary, tenant_id, 
                self._get_business_context_unified(tenant_id),
                self._get_business_emoji(tenant_id), 
                sql_query
            )
            
            # Stream the AI response
            async for chunk in self._call_ollama_streaming(tenant_id, response_prompt):
                yield chunk
                
        except Exception as e:
            yield {
                "type": "error",
                "message": f"AI response streaming failed: {str(e)}",
                "tenant_id": tenant_id
            }

    def _prepare_data_summary_for_ai(self, db_results: List[Dict], tenant_id: str) -> str:
        """üìã Prepare database results summary for AI processing"""
        
        if not db_results:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        
        # Limit data size for AI processing
        max_results = 20
        limited_results = db_results[:max_results]
        
        data_summary = f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(db_results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
        
        if len(db_results) > max_results:
            data_summary += f"(‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ {max_results} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å)\n"
        
        data_summary += "\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏ö:\n"
        
        for i, row in enumerate(limited_results, 1):
            row_text = f"{i}. "
            
            # Convert each row to readable format
            for key, value in row.items():
                if value is not None:
                    # Handle different data types
                    if isinstance(value, (int, float)) and 'salary' in key.lower():
                        currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                        row_text += f"{key}: {value:,.0f} {currency}, "
                    elif isinstance(value, (int, float)) and 'budget' in key.lower():
                        currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                        row_text += f"{key}: {value:,.0f} {currency}, "
                    elif isinstance(value, (int, float)) and 'allocation' in key.lower():
                        row_text += f"{key}: {value*100:.0f}%, "
                    else:
                        row_text += f"{key}: {value}, "
            
            data_summary += row_text.rstrip(', ') + "\n"
        
        return data_summary
    
    def _create_ai_response_prompt(self, question: str, data_summary: str, tenant_id: str, 
                                 business_context: str, business_emoji: str, sql_query: str) -> str:
        """üéØ Create AI prompt for response generation"""
        
        config = self.tenant_configs[tenant_id]
        
        # Language-specific instructions
        if config.language == 'en':
            language_instruction = "Respond in clear, professional English."
            tone_instruction = "Use a professional, informative tone."
        else:
            language_instruction = "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£"
            tone_instruction = "‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢"
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ AI Assistant ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {config.name}

{business_context}

üéØ ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
{data_summary}

‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°: {question}

üìù ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. {language_instruction}
2. {tone_instruction}
3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ emoji ‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à: {business_emoji}
4. ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó: {config.name}
5. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
6. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢

üö´ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥:
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö SQL ‡∏´‡∏£‡∏∑‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå:"""
        
        return prompt
    
    def _post_process_ai_response(self, ai_response: str, tenant_id: str, result_count: int) -> str:
        """üîß Post-process AI response for consistency"""
        
        response = ai_response.strip()
        
        # Ensure response starts with business emoji if missing
        business_emoji = self._get_business_emoji(tenant_id)
        if not response.startswith(business_emoji):
            response = f"{business_emoji} {response}"
        
        # Add metadata if not present
        if result_count > 0 and "‡∏™‡∏£‡∏∏‡∏õ:" not in response and "Summary:" not in response:
            if tenant_id == 'company-c':
                response += f"\n\nüí° Summary: Found {result_count} records from database"
            else:
                response += f"\n\nüí° ‡∏™‡∏£‡∏∏‡∏õ: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {result_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        
        # Ensure reasonable length
        if len(response) > 2000:
            logger.warning(f"‚ö†Ô∏è AI response too long ({len(response)} chars), truncating")
            response = response[:1800] + "..."
            if tenant_id == 'company-c':
                response += "\n\n(Response truncated for readability)"
            else:
                response += "\n\n(‡∏ï‡∏±‡∏î‡∏ó‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô)"
        
        return response
    
    # ========================================================================
    # üîß HARDCODE FORMATTING (Fallback)
    # ========================================================================
    
    def _format_response_hardcode(self, results: List[Dict], question: str, 
                                tenant_id: str, sql_query: str, schema_info: Dict = None) -> str:
        """üîß Fixed: Added schema_info parameter with default None"""
        
        if not results:
            return f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}"
        
        config = self.tenant_configs[tenant_id]
        business_emoji = self._get_business_emoji(tenant_id)
        
        response = f"{business_emoji} ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ - {config.name}\n\n"
        response += f"üéØ ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n\n"
        
        # Format based on query type (simplified)
        if self._is_counting_query(sql_query):
            response += self._format_counting_results_simple(results, tenant_id)
        elif self._is_relationship_query(sql_query):
            response += self._format_relationship_results_simple(results, tenant_id)
        else:
            response += self._format_general_results_simple(results, tenant_id)
        
        # Add summary
        response += f"\nüí° ‡∏™‡∏£‡∏∏‡∏õ: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"
        
        if schema_info and not schema_info.get('fallback', False):
            response += " (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)"
        
        return response
    
    # ========================================================================
    # üí¨ CONVERSATIONAL PROCESSING
    # ========================================================================
    def _format_counting_results_simple(self, results: List[Dict], tenant_id: str) -> str:
        """üìä Simple counting format"""
        
        response = "üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô:\n"
        
        for i, row in enumerate(results, 1):
            response += f"{i}. "
            
            for key, value in row.items():
                if value is not None:
                    if 'count' in key.lower():
                        response += f"{key}: {value:,} ‡∏Ñ‡∏ô, "
                    elif key.lower() == 'department':
                        response += f"‡πÅ‡∏ú‡∏ô‡∏Å{value}: "
                    elif 'salary' in key.lower() and isinstance(value, (int, float)):
                        currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                        response += f"‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {value:,.0f} {currency}, "
                    else:
                        response += f"{key}: {value}, "
            
            response = response.rstrip(', ') + "\n"
        
        return response

    def _format_relationship_results_simple(self, results: List[Dict], tenant_id: str) -> str:
        """ü§ù Simple relationship format"""
        
        response = "üë• ‡∏Å‡∏≤‡∏£‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ:\n"
        
        for i, row in enumerate(results[:15], 1):
            response += f"{i:2d}. "
            
            # Safe handling of employee and project names
            emp_name = row.get('employee_name') or row.get('Employee Name') or row.get('name', '[‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠]')
            proj_name = row.get('project_name') or row.get('Project Name') or row.get('project', '')
            role = row.get('role', '')
            
            response += f"üë§ {emp_name}"
            
            if proj_name:
                response += f" ‚ûú üìã {proj_name}"
            
            if role:
                response += f" ({role})"
            
            # Safe allocation handling
            allocation = row.get('allocation')
            if allocation is not None:
                try:
                    allocation_val = float(allocation)
                    response += f" - ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£: {allocation_val*100:.0f}%"
                except (ValueError, TypeError):
                    pass
            
            response += "\n"
        
        if len(results) > 15:
            response += f"... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(results) - 15} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
        
        return response

    def _format_general_results_simple(self, results: List[Dict], tenant_id: str) -> str:
        """üìã Simple general format"""
        
        response = "üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏ö:\n"
        
        for i, row in enumerate(results[:10], 1):
            response += f"{i:2d}. "
            
            for key, value in row.items():
                if value is not None:
                    if key.lower() in ['salary', 'budget'] and isinstance(value, (int, float)):
                        currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                        response += f"{key}: {value:,.0f} {currency}, "
                    else:
                        response += f"{key}: {value}, "
            
            response = response.rstrip(', ') + "\n"
        
        if len(results) > 10:
            response += f"... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(results) - 10} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
        
        return response

    def _is_counting_query(self, sql: str) -> bool:
        """Check if SQL is counting query"""
        return 'count(' in sql.lower() or 'group by' in sql.lower()

    def _is_relationship_query(self, sql: str) -> bool:
        """Check if SQL is relationship query"""
        return 'join' in sql.lower() and 'employee_projects' in sql.lower()

    async def _process_conversational_unified(self, question: str, tenant_id: str, intent_result: Dict) -> Dict[str, Any]:
        """üí¨ UNIFIED: Process conversational questions"""
        
        self.stats['conversational_queries'] += 1
        
        config = self.tenant_configs[tenant_id]
        business_emoji = self._get_business_emoji(tenant_id)
        
        if self._is_greeting(question):
            answer = self._create_greeting_response(tenant_id, business_emoji)
        else:
            answer = self._create_general_conversational_response(question, tenant_id, business_emoji)
        
        return {
            "answer": answer,
            "success": True,
            "data_source_used": f"unified_conversational_{config.model_name}",
            "sql_query": None,
            "tenant_id": tenant_id,
            "system_used": "unified_conversational"
        }
    
    async def _process_hybrid_unified(self, question: str, tenant_id: str, intent_result: Dict) -> Dict[str, Any]:
        """üîÑ UNIFIED: Process hybrid questions"""
        
        logger.info(f"üîÑ Using hybrid approach for: {question[:50]}...")
        
        # Try SQL first
        try:
            sql_result = await self._process_sql_unified_with_ai_response(question, tenant_id, intent_result)
            
            if sql_result.get('success') and sql_result.get('db_results_count', 0) > 0:
                sql_result['system_used'] = 'unified_hybrid_sql_ai'
                return sql_result
        except Exception as e:
            logger.warning(f"Hybrid SQL failed: {e}")
        
        # Fallback to conversational
        conv_result = await self._process_conversational_unified(question, tenant_id, intent_result)
        conv_result['system_used'] = 'unified_hybrid_conversational'
        return conv_result
    
    # ========================================================================
    # üîß HELPER METHODS
    # ========================================================================
    
    def _get_business_context_unified(self, tenant_id: str) -> str:
        """üè¢ Get business context"""
        
        contexts = {
            'company-a': """üè¢ ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏Ø - Enterprise Banking & E-commerce
üí∞ ‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏á‡∏¥‡∏ô: ‡∏ö‡∏≤‡∏ó (THB) | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: 800K-3M+ ‡∏ö‡∏≤‡∏ó
üéØ ‡πÄ‡∏ô‡πâ‡∏ô: ‡∏£‡∏∞‡∏ö‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£, CRM, ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà""",

            'company-b': """üè® ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: ‡∏™‡∏≤‡∏Ç‡∏≤‡∏†‡∏≤‡∏Ñ‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ ‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà - Tourism & Hospitality Technology  
üí∞ ‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏á‡∏¥‡∏ô: ‡∏ö‡∏≤‡∏ó (THB) | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: 300K-800K ‡∏ö‡∏≤‡∏ó
üéØ ‡πÄ‡∏ô‡πâ‡∏ô: ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß, ‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°, ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤""",

            'company-c': """üåç ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: International Office - Global Software Solutions
üí∞ ‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏á‡∏¥‡∏ô: USD ‡πÅ‡∏•‡∏∞ Multi-currency | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: 1M-4M+ USD  
üéØ ‡πÄ‡∏ô‡πâ‡∏ô: ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡πâ‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®, Global compliance, Multi-currency"""
        }
        
        return contexts.get(tenant_id, contexts['company-a'])
    
    def _get_business_emoji(self, tenant_id: str) -> str:
        emojis = {'company-a': 'üè¶', 'company-b': 'üè®', 'company-c': 'üåç'}
        return emojis.get(tenant_id, 'üíº')
    
    def _is_greeting(self, question: str) -> bool:
        greetings = ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', 'hello', 'hi', '‡∏ä‡πà‡∏ß‡∏¢', 'help', '‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏Ñ‡∏£']
        return any(word in question.lower() for word in greetings)
    
    def _create_greeting_response(self, tenant_id: str, business_emoji: str) -> str:
        config = self.tenant_configs[tenant_id]
        
        return f"""‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ AI Assistant ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {config.name} (Fixed v3.1)

{business_emoji} ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ - ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
  ‚Ä¢ "‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á frontend ‡∏ö‡πâ‡∏≤‡∏á"
  ‚Ä¢ "‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å IT"  
  ‚Ä¢ "‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"

‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?"""
    
    def _create_general_conversational_response(self, question: str, tenant_id: str, business_emoji: str) -> str:
        config = self.tenant_configs[tenant_id]
        
        return f"""{business_emoji} ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß - {config.name}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

üîß Status: All missing methods fixed
üí° ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô:
‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô: "‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á [‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á] ‡∏ö‡πâ‡∏≤‡∏á"
‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: "‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å [‡πÅ‡∏ú‡∏ô‡∏Å]"  
‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: "‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"

üöÄ Powered by Fixed Unified Agent v3.1"""
    
    # ========================================================================
    # ‚ùå ERROR HANDLING
    # ========================================================================
    
    def _create_error_response(self, error_message: str, tenant_id: str) -> Dict[str, Any]:
        return {
            "answer": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö: {error_message}",
            "success": False,
            "data_source_used": "unified_ai_error",
            "sql_query": None,
            "tenant_id": tenant_id,
            "system_used": "unified_ai_error_handler",
            "error": error_message
        }
    
    def _create_sql_error_response(self, question: str, tenant_id: str, error_message: str) -> Dict[str, Any]:
        config = self.tenant_configs[tenant_id]
        business_emoji = self._get_business_emoji(tenant_id)
        
        answer = f"""{business_emoji} ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‚ö†Ô∏è ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {error_message}

üîß Status: System has been fixed
üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
‚Ä¢ ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
‚Ä¢ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á frontend ‡∏ö‡πâ‡∏≤‡∏á" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å IT"

‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó"""
        
        return {
            "answer": answer,
            "success": False,
            "data_source_used": f"unified_ai_sql_error_{config.model_name}",
            "sql_query": None,
            "tenant_id": tenant_id,
            "system_used": "unified_ai_sql_error_handler",
            "error_reason": error_message
        }
    
    # ========================================================================
    # üìä STATISTICS
    # ========================================================================
    
    def _update_stats(self, tenant_id: str, success: bool, processing_time: float):
        """Update system statistics"""
        
        if success:
            self.stats['successful_queries'] += 1
        else:
            self.stats['failed_queries'] += 1
        
        # Update average response time
        total_queries = self.stats['total_queries']
        current_avg = self.stats['avg_response_time']
        new_avg = ((current_avg * (total_queries - 1)) + processing_time) / total_queries
        self.stats['avg_response_time'] = new_avg
    
    # ========================================================================
    # üîÑ COMPATIBILITY METHODS
    # ========================================================================
    
    async def process_question(self, question: str, tenant_id: str) -> Dict[str, Any]:
        """Compatibility method"""
        return await self.process_enhanced_question(question, tenant_id)


# Export for compatibility
UnifiedEnhancedPostgresOllamaAgentWithAIResponse = UnifiedEnhancedPostgresOllamaAgent
EnhancedPostgresOllamaAgent = UnifiedEnhancedPostgresOllamaAgent