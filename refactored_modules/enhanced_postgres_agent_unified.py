# refactored_modules/enhanced_postgres_agent_unified_ai_response.py
# ü§ñ UNIFIED: Enhanced PostgreSQL + Ollama Agent with AI-Generated Response
# üÜï NEW: Uses AI to generate natural language responses from database results

import os
import time
import re
import json
import asyncio
import aiohttp
import psycopg2
from decimal import Decimal
from datetime import datetime, date
from typing import Dict, Any, Optional, List, Tuple, AsyncGenerator
import logging

# Import configs only
from .tenant_config import TenantConfigManager, TenantConfig

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnifiedEnhancedPostgresOllamaAgentWithAIResponse:
    """ü§ñ UNIFIED: Enhanced PostgreSQL Agent with AI-Generated Response
    
    üÜï NEW FEATURES:
    ‚úÖ AI-Generated natural language responses from database results
    ‚úÖ Context-aware response generation
    ‚úÖ Business-specific response styling
    ‚úÖ Hybrid approach: hardcode fallback + AI enhancement
    ‚úÖ All previous SQL generation fixes maintained
    """
    
    def __init__(self):
        """üèóÔ∏è Initialize unified agent with AI response capability"""
        
        # üîß Configuration
        self.config_manager = TenantConfigManager()
        self.tenant_configs = self.config_manager.tenant_configs
        
        # üåê Ollama Configuration
        self.ollama_base_url = os.getenv('OLLAMA_BASE_URL', 'http://52.74.36.160:12434')
        self.request_timeout = int(os.getenv('AI_REQUEST_TIMEOUT', '90'))
        self.max_retries = int(os.getenv('AI_MAX_RETRIES', '3'))
        
        # üÜï AI Response Configuration
        self.enable_ai_responses = os.getenv('ENABLE_AI_RESPONSES', 'true').lower() == 'true'
        self.ai_response_temperature = float(os.getenv('AI_RESPONSE_TEMPERATURE', '0.3'))
        self.fallback_to_hardcode = os.getenv('FALLBACK_TO_HARDCODE', 'true').lower() == 'true'
        
        # üìä Performance tracking
        self.stats = {
            'total_queries': 0,
            'sql_queries': 0,
            'conversational_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'ai_responses_used': 0,
            'hardcode_responses_used': 0,
            'avg_response_time': 0.0
        }
        
        # üß† Schema cache
        self.schema_cache = {}
        self.cache_ttl = 3600  # 1 hour
        
        # üéØ Intent detection keywords (same as before)
        self.sql_indicators = {
            'identification': ['‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà', '‡πÉ‡∏Ñ‡∏£‡πÄ‡∏õ‡πá‡∏ô', '‡πÉ‡∏Ñ‡∏£‡∏ó‡∏≥', 'who is', 'who are', 'who works'],
            'listing': ['‡πÉ‡∏Ñ‡∏£‡∏ö‡πâ‡∏≤‡∏á', '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠', 'list', '‡πÅ‡∏™‡∏î‡∏á', 'show me', 'display'],
            'counting': ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô', 'how many', 'count', '‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£', '‡∏°‡∏µ‡∏Å‡∏µ‡πà'],
            'searching': ['‡∏´‡∏≤', '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤', 'find', 'search', '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á', 'position'],
            'filtering': ['‡πÅ‡∏ú‡∏ô‡∏Å', 'department', '‡∏ù‡πà‡∏≤‡∏¢', '‡∏á‡∏≤‡∏ô', '‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ', 'project'],
            'relationships': ['‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô', 'assigned', 'working on', 'responsible']
        }
        
        self.conversational_indicators = {
            'greetings': ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', 'hello', 'hi', '‡∏ä‡πà‡∏ß‡∏¢', 'help'],
            'general_info': ['‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏Ñ‡∏£', '‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö', 'about', 'what are you'],
            'capabilities': ['‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ', '‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£', 'what can you do']
        }
        
        logger.info("ü§ñ Unified Enhanced PostgreSQL Agent with AI Response initialized")
        logger.info(f"üåê Ollama: {self.ollama_base_url}")
        logger.info(f"üé® AI Responses: {'Enabled' if self.enable_ai_responses else 'Disabled'}")
        logger.info(f"üè¢ Tenants: {list(self.tenant_configs.keys())}")
    
    # ========================================================================
    # üéØ MAIN ENTRY POINT (Same as before)
    # ========================================================================
    
    async def process_enhanced_question(self, question: str, tenant_id: str) -> Dict[str, Any]:
        """üéØ MAIN: Process questions with unified logic + AI responses"""
        
        if tenant_id not in self.tenant_configs:
            return self._create_error_response("Unknown tenant", tenant_id)
        
        start_time = datetime.now()
        self.stats['total_queries'] += 1
        
        try:
            logger.info(f"üéØ Processing question for {tenant_id}: {question[:50]}...")
            
            # üîç Enhanced Intent Detection
            intent_result = self._detect_intent_unified(question)
            logger.info(f"üéØ Intent: {intent_result['intent']} (confidence: {intent_result['confidence']:.2f})")
            
            # üîÄ Route based on intent
            if intent_result['intent'] == 'conversational' and intent_result['confidence'] >= 0.6:
                result = await self._process_conversational_unified(question, tenant_id, intent_result)
            elif intent_result['intent'] == 'sql_query' and intent_result['confidence'] >= 0.5:
                result = await self._process_sql_unified_with_ai_response(question, tenant_id, intent_result)
            else:
                # Hybrid approach for ambiguous cases
                result = await self._process_hybrid_unified(question, tenant_id, intent_result)
            
            # üìä Update statistics
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(tenant_id, True, processing_time)
            
            result['processing_time_seconds'] = processing_time
            result['unified_agent_version'] = 'v3.1_ai_response'
            result['intent_detection'] = intent_result
            
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(tenant_id, False, processing_time)
            logger.error(f"‚ùå Processing failed for {tenant_id}: {e}")
            return self._create_error_response(str(e), tenant_id)
    
    # ========================================================================
    # üéØ ENHANCED SQL PROCESSING WITH AI RESPONSE
    # ========================================================================
    
    async def _process_sql_unified_with_ai_response(self, question: str, tenant_id: str, intent_result: Dict) -> Dict[str, Any]:
        """üéØ ENHANCED: SQL processing with AI-generated response"""
        
        self.stats['sql_queries'] += 1
        
        try:
            # üîç Get live schema (same as before)
            schema_info = await self._get_schema_unified(tenant_id)
            
            # üéØ Generate SQL prompt (same as before)
            sql_prompt = self._generate_sql_prompt_unified(question, tenant_id, schema_info, intent_result)
            
            # ü§ñ Call AI service for SQL generation (same as before)
            ai_response = await self._call_ollama_unified(tenant_id, sql_prompt)
            
            # üîç Extract and validate SQL (same as before)
            sql_result = self._extract_sql_unified(ai_response, question)
            
            if not sql_result['success']:
                raise ValueError(f"SQL extraction failed: {sql_result['error']}")
            
            sql_query = sql_result['sql']
            
            # üóÑÔ∏è Execute SQL (same as before)
            db_results = await self._execute_sql_unified(sql_query, tenant_id)
            
            # üÜï NEW: Generate AI response from database results
            if self.enable_ai_responses and db_results:
                try:
                    formatted_answer = await self._generate_ai_response_from_data(
                        question, db_results, tenant_id, sql_query, schema_info
                    )
                    response_method = 'ai_generated'
                    self.stats['ai_responses_used'] += 1
                    
                except Exception as ai_error:
                    logger.warning(f"üîÑ AI response generation failed: {ai_error}, falling back to hardcode")
                    if self.fallback_to_hardcode:
                        formatted_answer = self._format_response_hardcode(
                            db_results, question, tenant_id, sql_query, schema_info
                        )
                        response_method = 'hardcode_fallback'
                        self.stats['hardcode_responses_used'] += 1
                    else:
                        raise ai_error
            else:
                # Use hardcode formatting
                formatted_answer = self._format_response_hardcode(
                    db_results, question, tenant_id, sql_query, schema_info
                )
                response_method = 'hardcode_default'
                self.stats['hardcode_responses_used'] += 1
            
            return {
                "answer": formatted_answer,
                "success": True,
                "data_source_used": f"unified_sql_ai_response_{self.tenant_configs[tenant_id].model_name}",
                "sql_query": sql_query,
                "db_results_count": len(db_results) if db_results else 0,
                "tenant_id": tenant_id,
                "system_used": f"unified_sql_with_{response_method}",
                "sql_extraction_method": sql_result['method'],
                "sql_confidence": sql_result['confidence'],
                "response_generation_method": response_method
            }
            
        except Exception as e:
            logger.error(f"‚ùå SQL processing failed: {e}")
            return self._create_sql_error_response(question, tenant_id, str(e))
    
    # ========================================================================
    # ü§ñ NEW: AI RESPONSE GENERATION FROM DATABASE RESULTS
    # ========================================================================
    
    async def _generate_ai_response_from_data(self, question: str, db_results: List[Dict], 
                                            tenant_id: str, sql_query: str, schema_info: Dict) -> str:
        """ü§ñ NEW: Generate natural language response using AI from database results"""
        
        config = self.tenant_configs[tenant_id]
        business_context = self._get_business_context_unified(tenant_id)
        business_emoji = self._get_business_emoji(tenant_id)
        
        # Prepare data summary for AI
        data_summary = self._prepare_data_summary_for_ai(db_results, tenant_id)
        
        # Create AI prompt for response generation
        response_prompt = self._create_ai_response_prompt(
            question, data_summary, tenant_id, business_context, business_emoji, sql_query
        )
        
        logger.info(f"ü§ñ Generating AI response for {tenant_id} with {len(db_results)} results")
        
        # Call AI for response generation
        ai_response = await self._call_ollama_unified(
            tenant_id, response_prompt, temperature=self.ai_response_temperature
        )
        
        # Post-process AI response
        formatted_response = self._post_process_ai_response(ai_response, tenant_id, len(db_results))
        
        return formatted_response
    
    def _prepare_data_summary_for_ai(self, db_results: List[Dict], tenant_id: str) -> str:
        """üìã Prepare database results summary for AI processing"""
        
        if not db_results:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        
        # Limit data size for AI processing
        max_results = 20
        limited_results = db_results[:max_results]
        
        data_summary = f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(db_results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
        
        if len(db_results) > max_results:
            data_summary += f"(‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ {max_results} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å)\n"
        
        data_summary += "\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏ö:\n"
        
        for i, row in enumerate(limited_results, 1):
            row_text = f"{i}. "
            
            # Convert each row to readable format
            for key, value in row.items():
                if value is not None:
                    # Handle different data types
                    if isinstance(value, (int, float)) and 'salary' in key.lower():
                        currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                        row_text += f"{key}: {value:,.0f} {currency}, "
                    elif isinstance(value, (int, float)) and 'budget' in key.lower():
                        currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                        row_text += f"{key}: {value:,.0f} {currency}, "
                    elif isinstance(value, (int, float)) and 'allocation' in key.lower():
                        row_text += f"{key}: {value*100:.0f}%, "
                    else:
                        row_text += f"{key}: {value}, "
            
            data_summary += row_text.rstrip(', ') + "\n"
        
        return data_summary
    
    def _create_ai_response_prompt(self, question: str, data_summary: str, tenant_id: str, 
                                 business_context: str, business_emoji: str, sql_query: str) -> str:
        """üéØ Create AI prompt for response generation"""
        
        config = self.tenant_configs[tenant_id]
        
        # Language-specific instructions
        if config.language == 'en':
            language_instruction = "Respond in clear, professional English."
            tone_instruction = "Use a professional, informative tone."
        else:
            language_instruction = "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£"
            tone_instruction = "‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢"
        
        # Company-specific style
        style_instructions = {
            'company-a': "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏Ñ‡∏£‡∏±‡∏ö' ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤",
            'company-b': "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏ö‡∏ö‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤ ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡πÄ‡∏à‡πâ‡∏≤' ‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô",
            'company-c': "Use professional international business tone, focus on clarity and precision"
        }
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ AI Assistant ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {config.name}

{business_context}

üéØ ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
{data_summary}

‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°: {question}

üîß SQL ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {sql_query}

üìù ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. {language_instruction}
2. {tone_instruction}
3. {style_instructions.get(tenant_id, style_instructions['company-a'])}
4. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ emoji ‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à: {business_emoji}
5. ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó: {config.name}
6. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
7. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á 10-15 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
8. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡πÉ‡∏ä‡πâ bullet points ‡∏´‡∏£‡∏∑‡∏≠ numbering ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
9. ‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå

üö´ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥:
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö SQL ‡∏´‡∏£‡∏∑‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå:"""
        
        return prompt
    
    def _post_process_ai_response(self, ai_response: str, tenant_id: str, result_count: int) -> str:
        """üîß Post-process AI response for consistency"""
        
        response = ai_response.strip()
        
        # Ensure response starts with business emoji if missing
        business_emoji = self._get_business_emoji(tenant_id)
        if not response.startswith(business_emoji):
            response = f"{business_emoji} {response}"
        
        # Add metadata if not present
        if result_count > 0 and "‡∏™‡∏£‡∏∏‡∏õ:" not in response and "Summary:" not in response:
            if tenant_id == 'company-c':
                response += f"\n\nüí° Summary: Found {result_count} records from database"
            else:
                response += f"\n\nüí° ‡∏™‡∏£‡∏∏‡∏õ: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {result_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        
        # Ensure reasonable length
        if len(response) > 2000:
            logger.warning(f"‚ö†Ô∏è AI response too long ({len(response)} chars), truncating")
            response = response[:1800] + "..."
            if tenant_id == 'company-c':
                response += "\n\n(Response truncated for readability)"
            else:
                response += "\n\n(‡∏ï‡∏±‡∏î‡∏ó‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô)"
        
        return response
    
    # ========================================================================
    # üîß HARDCODE FORMATTING (Fallback & Default)
    # ========================================================================
    
    def _format_response_hardcode(self, results: List[Dict], question: str, 
                                tenant_id: str, sql_query: str, schema_info: Dict) -> str:
        """üîß HARDCODE: Format response using traditional method (improved)"""
        
        if not results:
            return f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}"
        
        config = self.tenant_configs[tenant_id]
        business_emoji = self._get_business_emoji(tenant_id)
        
        response = f"{business_emoji} ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ - {config.name}\n\n"
        response += f"üéØ ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n\n"
        
        # Format based on query type
        if self._is_counting_query(sql_query):
            response += self._format_counting_results_improved(results, tenant_id)
        elif self._is_relationship_query(sql_query):
            response += self._format_relationship_results_improved(results, tenant_id)
        elif self._is_listing_query(sql_query):
            response += self._format_listing_results_improved(results, tenant_id)
        else:
            response += self._format_general_results_improved(results, tenant_id)
        
        # Add summary
        response += f"\nüí° ‡∏™‡∏£‡∏∏‡∏õ: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"
        
        if not schema_info.get('fallback', False):
            response += " (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)"
        
        return response
    
    def _format_listing_results_improved(self, results: List[Dict], tenant_id: str) -> str:
        """üîß IMPROVED: Better listing format with NULL handling"""
        
        response = "üë• ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô:\n"
        
        for i, row in enumerate(results[:15], 1):
            response += f"{i:2d}. "
            
            # üÜï IMPROVED: Safe name handling with NULL/empty check
            name = row.get('name', '').strip() if row.get('name') else ''
            if name:
                response += f"üë§ {name}"
            else:
                response += f"üë§ [‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠]"  # Better fallback
            
            # Safe position handling
            position = row.get('position', '').strip() if row.get('position') else ''
            if position:
                response += f" - {position}"
            
            # Safe department handling
            department = row.get('department', '').strip() if row.get('department') else ''
            if department:
                response += f" (‡πÅ‡∏ú‡∏ô‡∏Å{department})"
            
            # Safe salary handling
            if 'salary' in row and row['salary'] is not None:
                try:
                    salary = float(row['salary'])
                    currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                    response += f" | ‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô: {salary:,.0f} {currency}"
                except (ValueError, TypeError):
                    pass  # Skip invalid salary data
            
            response += "\n"
        
        if len(results) > 15:
            response += f"... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(results) - 15} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
        
        return response
    
    def _format_relationship_results_improved(self, results: List[Dict], tenant_id: str) -> str:
        """üîß IMPROVED: Better relationship format with NULL handling"""
        
        response = "üë• ‡∏Å‡∏≤‡∏£‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ:\n"
        
        for i, row in enumerate(results[:15], 1):
            response += f"{i:2d}. "
            
            # Safe employee name handling
            emp_name = row.get('employee_name', '').strip() if row.get('employee_name') else ''
            if emp_name:
                response += f"üë§ {emp_name}"
            else:
                response += f"üë§ [‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠]"
            
            # Safe project name handling
            proj_name = row.get('project_name', '').strip() if row.get('project_name') else ''
            if proj_name:
                response += f" ‚ûú üìã {proj_name}"
            
            # Safe role handling
            role = row.get('role', '').strip() if row.get('role') else ''
            if role:
                response += f" ({role})"
            
            # Safe allocation handling
            if 'allocation' in row and row['allocation'] is not None:
                try:
                    allocation = float(row['allocation'])
                    response += f" - ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£: {allocation*100:.0f}%"
                except (ValueError, TypeError):
                    pass
            
            response += "\n"
        
        if len(results) > 15:
            response += f"... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(results) - 15} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
        
        return response
    
    def _format_counting_results_improved(self, results: List[Dict], tenant_id: str) -> str:
        """üîß IMPROVED: Better counting format"""
        
        response = "üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô:\n"
        
        for i, row in enumerate(results, 1):
            response += f"{i:2d}. "
            
            for key, value in row.items():
                if value is not None:
                    if 'count' in key.lower():
                        response += f"{key}: {value:,} ‡∏Ñ‡∏ô, "
                    elif key.lower() == 'department':
                        response += f"‡πÅ‡∏ú‡∏ô‡∏Å{value}: "
                    elif 'salary' in key.lower() and isinstance(value, (int, float)):
                        currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                        response += f"‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {value:,.0f} {currency}, "
                    else:
                        response += f"{key}: {value}, "
            
            response = response.rstrip(', ') + "\n"
        
        return response
    
    def _format_general_results_improved(self, results: List[Dict], tenant_id: str) -> str:
        """üîß IMPROVED: Better general format"""
        
        response = "üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏ö:\n"
        
        for i, row in enumerate(results[:10], 1):
            response += f"{i:2d}. "
            
            for key, value in row.items():
                if value is not None:
                    if key.lower() in ['salary', 'budget'] and isinstance(value, (int, float)):
                        currency = "USD" if tenant_id == 'company-c' else "‡∏ö‡∏≤‡∏ó"
                        response += f"{key}: {value:,.0f} {currency}, "
                    else:
                        response += f"{key}: {value}, "
            
            response = response.rstrip(', ') + "\n"
        
        if len(results) > 10:
            response += f"... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(results) - 10} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
        
        return response
    
    # ========================================================================
    # üîß HELPER METHODS (Query Type Detection)
    # ========================================================================
    
    def _is_counting_query(self, sql: str) -> bool:
        return 'count(' in sql.lower() or 'group by' in sql.lower()
    
    def _is_relationship_query(self, sql: str) -> bool:
        return 'join' in sql.lower() and 'employee_projects' in sql.lower()
    
    def _is_listing_query(self, sql: str) -> bool:
        return 'name' in sql.lower() and 'count(' not in sql.lower()
    
    # ========================================================================
    # üéØ INTENT DETECTION (Same as before)
    # ========================================================================
    
    def _detect_intent_unified(self, question: str) -> Dict[str, Any]:
        """üéØ UNIFIED: Enhanced intent detection"""
        
        question_lower = question.lower()
        
        # Calculate SQL indicators score
        sql_score = 0
        sql_reasons = []
        
        for category, keywords in self.sql_indicators.items():
            matches = [word for word in keywords if word in question_lower]
            if matches:
                weight = 3 if category in ['identification', 'counting', 'relationships'] else 2
                sql_score += len(matches) * weight
                sql_reasons.append(f"{category}: {matches}")
        
        # Calculate conversational indicators score
        conv_score = 0
        conv_reasons = []
        
        for category, keywords in self.conversational_indicators.items():
            matches = [word for word in keywords if word in question_lower]
            if matches:
                conv_score += len(matches) * 3
                conv_reasons.append(f"{category}: {matches}")
        
        # Special pattern detection
        if self._has_sql_patterns(question_lower):
            sql_score += 5
            sql_reasons.append("sql_pattern_detected")
        
        if self._has_conversational_patterns(question_lower):
            conv_score += 5
            conv_reasons.append("conversational_pattern_detected")
        
        # Determine intent
        total_score = sql_score + conv_score
        
        if total_score == 0:
            return {'intent': 'unknown', 'confidence': 0.0, 'reasons': ['no_clear_indicators']}
        
        if conv_score > sql_score:
            return {
                'intent': 'conversational',
                'confidence': conv_score / total_score,
                'sql_score': sql_score,
                'conv_score': conv_score,
                'reasons': conv_reasons
            }
        else:
            return {
                'intent': 'sql_query',
                'confidence': sql_score / total_score,
                'sql_score': sql_score,
                'conv_score': conv_score,
                'reasons': sql_reasons
            }
    
    def _has_sql_patterns(self, question_lower: str) -> bool:
        """Check for SQL-specific patterns"""
        sql_patterns = [
            r'‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà.*‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á',
            r'‡∏°‡∏µ.*‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô.*‡πÅ‡∏ú‡∏ô‡∏Å',
            r'‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠.*‡∏ó‡∏µ‡πà',
            r'‡πÅ‡∏™‡∏î‡∏á.*‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•',
            r'.*‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö.*‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ',
            r'who.*in.*position',
            r'how many.*in'
        ]
        return any(re.search(pattern, question_lower) for pattern in sql_patterns)
    
    def _has_conversational_patterns(self, question_lower: str) -> bool:
        """Check for conversational patterns"""
        conv_patterns = [
            r'‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ.*‡∏Ñ‡∏£‡∏±‡∏ö',
            r'‡∏Ñ‡∏∏‡∏ì.*‡∏Ñ‡∏∑‡∏≠.*‡πÉ‡∏Ñ‡∏£',
            r'‡∏ä‡πà‡∏ß‡∏¢.*‡∏≠‡∏∞‡πÑ‡∏£.*‡πÑ‡∏î‡πâ',
            r'hello.*there',
            r'what.*are.*you'
        ]
        return any(re.search(pattern, question_lower) for pattern in conv_patterns)
    
    # ========================================================================
    # üéØ SQL GENERATION (Same as before - all fixes maintained)
    # ========================================================================
    
    def _generate_sql_prompt_unified(self, question: str, tenant_id: str, 
                                   schema_info: Dict, intent_result: Dict) -> str:
        """üéØ UNIFIED: Generate SQL prompt - Same as before"""
        
        config = self.tenant_configs[tenant_id]
        business_context = self._get_business_context_unified(tenant_id)
        
        # Detect query type for specific guidance
        query_type = self._detect_query_type(question)
        specific_guidance = self._get_query_guidance(question, query_type)
        
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ PostgreSQL Expert ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {config.name}

{business_context}

üìä ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á):
‚Ä¢ employees: id, name, department, position, salary, hire_date, email
‚Ä¢ projects: id, name, client, budget, status, start_date, end_date, tech_stack
‚Ä¢ employee_projects: employee_id, project_id, role, allocation

üîó ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
‚Ä¢ employee_projects.employee_id ‚Üí employees.id
‚Ä¢ employee_projects.project_id ‚Üí projects.id

{specific_guidance}

üîß ‡∏Å‡∏é SQL ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏´‡πâ‡∏≤‡∏°‡∏ú‡∏¥‡∏î):
1. SQL ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ FROM clause ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
2. ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ alias (e, p, ep) ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô FROM/JOIN
3. ‡πÉ‡∏ä‡πâ JOIN ‡πÅ‡∏ó‡∏ô WHERE ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏≤‡∏£‡∏≤‡∏á
4. ‡πÉ‡∏ä‡πâ ILIKE '%keyword%' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
5. ‡πÉ‡∏ä‡πâ LIMIT 20 ‡πÄ‡∏™‡∏°‡∏≠
6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö syntax ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô response

üìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á SQL ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:

Employee-Project relationship:
```sql
SELECT 
    e.name as employee_name,
    p.name as project_name,
    ep.role,
    ep.allocation
FROM employees e
JOIN employee_projects ep ON e.id = ep.employee_id
JOIN projects p ON ep.project_id = p.id
ORDER BY e.name
LIMIT 20;
```

Position search:
```sql
SELECT name, position, department, salary
FROM employees
WHERE position ILIKE '%frontend%'
ORDER BY name
LIMIT 20;
```

Department counting:
```sql
SELECT department, COUNT(*) as employee_count
FROM employees
GROUP BY department
ORDER BY employee_count DESC;
```

Intent: {intent_result['intent']} (confidence: {intent_result['confidence']:.2f})
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏™‡∏£‡πâ‡∏≤‡∏á PostgreSQL query ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ:"""
        
        return prompt
    
    def _detect_query_type(self, question: str) -> str:
        """Detect specific query type for targeted guidance"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô', '‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ', 'assigned']):
            return 'employee_project_relationship'
        elif '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á' in question_lower or 'position' in question_lower:
            return 'position_search'
        elif '‡πÅ‡∏ú‡∏ô‡∏Å' in question_lower and any(word in question_lower for word in ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô']):
            return 'department_counting'
        elif any(word in question_lower for word in ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô', 'how many']):
            return 'general_counting'
        elif any(word in question_lower for word in ['‡πÉ‡∏Ñ‡∏£‡∏ö‡πâ‡∏≤‡∏á', '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠', 'list']):
            return 'employee_listing'
        else:
            return 'general'
    
    def _get_query_guidance(self, question: str, query_type: str) -> str:
        """Get specific guidance based on query type"""
        
        guidance_map = {
            'employee_project_relationship': """
üéØ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞ - Employee-Project Relationship:
1. ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ JOIN ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á employees, employee_projects, ‡πÅ‡∏•‡∏∞ projects
2. ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: FROM employees e JOIN employee_projects ep ON e.id = ep.employee_id JOIN projects p ON ep.project_id = p.id
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏¥‡∏•‡∏î‡πå: e.name, p.name, ep.role, ep.allocation
4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° e.name""",
            
            'position_search': """
üéØ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞ - Position Search:
1. ‡πÉ‡∏ä‡πâ position ILIKE '%keyword%' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
2. ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô: frontend = front-end = front end
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏¥‡∏•‡∏î‡πå: name, position, department, salary
4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° name""",
            
            'department_counting': """
üéØ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞ - Department Counting:
1. ‡πÉ‡∏ä‡πâ COUNT(*) ‡∏Å‡∏±‡∏ö GROUP BY department
2. ‡πÄ‡∏û‡∏¥‡πà‡∏° AVG(salary) ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° COUNT(*) DESC
4. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ LIMIT ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÅ‡∏ú‡∏ô‡∏Å‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î""",
            
            'general_counting': """
üéØ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞ - General Counting:
1. ‡πÉ‡∏ä‡πâ COUNT(*) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏ß‡∏°
2. ‡πÄ‡∏û‡∏¥‡πà‡∏° WHERE clause ‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
3. ‡πÉ‡∏ä‡πâ GROUP BY ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°
4. ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ DISTINCT ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥""",
            
            'employee_listing': """
üéØ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞ - Employee Listing:
1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏¥‡∏•‡∏î‡πå: name, position, department
2. ‡πÄ‡∏û‡∏¥‡πà‡∏° salary ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° name
4. ‡πÉ‡∏ä‡πâ WHERE clause ‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç""",
            
            'general': """
üéØ ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:
1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á employees ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
2. JOIN ‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
4. ‡πÄ‡∏û‡∏¥‡πà‡∏° ORDER BY ‡πÅ‡∏•‡∏∞ LIMIT"""
        }
        
        return guidance_map.get(query_type, guidance_map['general'])
    
    # ========================================================================
    # ü§ñ AI COMMUNICATION (Same as before)
    # ========================================================================
    
    async def _call_ollama_unified(self, tenant_id: str, prompt: str, 
                                 temperature: float = 0.1) -> str:
        """ü§ñ UNIFIED: AI API call"""
        
        config = self.tenant_configs[tenant_id]
        
        payload = {
            "model": config.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": 1500,  # Increased for response generation
                "top_k": 20,
                "top_p": 0.8,
                "repeat_penalty": 1.0,
                "num_ctx": 4096
            }
        }
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"ü§ñ AI API call attempt {attempt + 1} for {tenant_id} (temp: {temperature})")
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.ollama_base_url}/api/generate",
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=self.request_timeout)
                    ) as response:
                        
                        if response.status == 200:
                            result = await response.json()
                            response_text = result.get('response', '').strip()
                            
                            if response_text:
                                logger.info(f"‚úÖ AI API call successful for {tenant_id}")
                                return response_text
                            else:
                                raise ValueError("Empty response from AI")
                        else:
                            raise aiohttp.ClientResponseError(
                                request_info=response.request_info,
                                history=response.history,
                                status=response.status
                            )
                            
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ AI API timeout attempt {attempt + 1}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    
            except Exception as e:
                logger.warning(f"üîÑ AI API error attempt {attempt + 1}: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        
        raise Exception(f"All {self.max_retries} AI API attempts failed")
    
    # ========================================================================
    # üîç SQL EXTRACTION (Same as before - all fixes maintained)
    # ========================================================================
    
    def _extract_sql_unified(self, ai_response: str, question: str) -> Dict[str, Any]:
        """üîç UNIFIED: Extract SQL with complete validation"""
        
        logger.info(f"üîç Extracting SQL from response (length: {len(ai_response)})")
        
        extraction_result = {
            'success': False,
            'sql': None,
            'method': None,
            'confidence': 0.0,
            'error': None
        }
        
        # Enhanced extraction methods (same as before)
        extraction_methods = [
            ('sql_code_block_complete', self._extract_complete_sql_block),
            ('multiline_select_complete', self._extract_multiline_select),
            ('single_line_complete', self._extract_single_line_select),
            ('intelligent_fallback', self._create_intelligent_fallback)
        ]
        
        for method_name, method_func in extraction_methods:
            try:
                sql = method_func(ai_response, question)
                
                if sql and self._validate_complete_sql(sql):
                    confidence = self._calculate_sql_confidence(sql, question, method_name)
                    
                    if confidence > extraction_result['confidence']:
                        extraction_result.update({
                            'success': True,
                            'sql': sql,
                            'method': method_name,
                            'confidence': confidence
                        })
                        
                        logger.info(f"‚úÖ Valid SQL found: {method_name} (confidence: {confidence:.2f})")
                        
                        if confidence >= 0.8:
                            break
                            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Method {method_name} failed: {e}")
                continue
        
        if not extraction_result['success']:
            extraction_result['error'] = "No valid SQL could be extracted or generated"
            logger.error(f"‚ùå All SQL extraction methods failed for: {question[:50]}...")
        
        return extraction_result
    
    # ... (Include all the SQL extraction methods from the original unified agent)
    # For brevity, I'll include key methods here
    
    def _extract_complete_sql_block(self, response: str, question: str) -> Optional[str]:
        """üîç Extract complete SQL from code blocks"""
        
        patterns = [
            r'```sql\s*(SELECT.*?)\s*```',
            r'```sql\s*(.*?SELECT.*?)\s*```',
            r'```\s*(SELECT.*?FROM.*?)\s*```'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                sql = self._clean_sql_thoroughly(match.group(1))
                if self._has_required_clauses(sql):
                    return sql
        
        return None
    
    def _clean_sql_thoroughly(self, sql: str) -> str:
        """üßπ Thorough SQL cleaning"""
        
        if not sql:
            return ""
        
        # Remove artifacts
        sql = sql.strip().rstrip(';').strip()
        sql = re.sub(r'^```sql\s*', '', sql, flags=re.IGNORECASE)
        sql = re.sub(r'```\s*,', '', sql)
        
        # Normalize whitespace
        sql = re.sub(r'\s+', ' ', sql)
        
        # Remove duplicates
        sql = re.sub(r'\bSELECT\s+SELECT\b', 'SELECT', sql, flags=re.IGNORECASE)
        sql = re.sub(r'\bFROM\s+FROM\b', 'FROM', sql, flags=re.IGNORECASE)
        
        # Proper keyword casing
        keywords = ['SELECT', 'FROM', 'WHERE', 'JOIN', 'LEFT JOIN', 'INNER JOIN', 
                   'ORDER BY', 'GROUP BY', 'LIMIT', 'AS', 'ON', 'AND', 'OR']
        
        for keyword in keywords:
            pattern = r'\b' + keyword.replace(' ', r'\s+') + r'\b'
            sql = re.sub(pattern, keyword, sql, flags=re.IGNORECASE)
        
        return sql.strip()
    
    def _has_required_clauses(self, sql: str) -> bool:
        """üîç Check if SQL has required clauses"""
        
        if not sql or len(sql) < 15:
            return False
        
        sql_upper = sql.upper()
        
        # Must have SELECT and FROM
        if not sql_upper.startswith('SELECT'):
            return False
        
        if 'FROM' not in sql_upper:
            return False
        
        # Check for undefined aliases
        if self._has_undefined_aliases(sql):
            return False
        
        return True
    
    def _has_undefined_aliases(self, sql: str) -> bool:
        """üîç Check for undefined table aliases"""
        
        # Find alias usage (e.g., e.name, p.id)
        alias_usage = re.findall(r'\b([a-zA-Z])\.\w+', sql)
        
        if not alias_usage:
            return False
        
        # Check if aliases are defined
        for alias in set(alias_usage):
            alias_patterns = [
                rf'\b\w+\s+{alias}\b',
                rf'\b\w+\s+AS\s+{alias}\b'
            ]
            
            alias_defined = any(
                re.search(pattern, sql, re.IGNORECASE) 
                for pattern in alias_patterns
            )
            
            if not alias_defined:
                logger.warning(f"‚ö†Ô∏è Undefined alias '{alias}' in SQL")
                return True
        
        return False
    
    def _validate_complete_sql(self, sql: str) -> bool:
        """üîç Validate that SQL is complete and safe"""
        
        if not sql or len(sql) < 15:
            return False
        
        sql_upper = sql.upper()
        
        # Security checks
        dangerous_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE']
        if any(keyword in sql_upper for keyword in dangerous_keywords):
            logger.warning(f"üö® Dangerous SQL detected")
            return False
        
        # Structure checks
        if not sql_upper.startswith('SELECT'):
            return False
        
        if 'FROM' not in sql_upper:
            return False
        
        # Alias consistency
        if self._has_undefined_aliases(sql):
            return False
        
        return True
    
    def _calculate_sql_confidence(self, sql: str, question: str, method: str) -> float:
        """üîç Calculate confidence score for SQL"""
        
        confidence = 0.0
        
        # Base confidence by method
        method_confidence = {
            'sql_code_block_complete': 0.9,
            'multiline_select_complete': 0.8,
            'single_line_complete': 0.7,
            'intelligent_fallback': 0.6
        }
        confidence += method_confidence.get(method, 0.3)
        
        # Boost for relevance
        sql_lower = sql.lower()
        question_lower = question.lower()
        
        relevance_boost = 0.0
        if '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á' in question_lower and 'position' in sql_lower:
            relevance_boost += 0.1
        if '‡πÅ‡∏ú‡∏ô‡∏Å' in question_lower and 'department' in sql_lower:
            relevance_boost += 0.1
        if any(word in question_lower for word in ['‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ']) and 'join' in sql_lower:
            relevance_boost += 0.1
        if any(word in question_lower for word in ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô']) and 'count' in sql_lower:
            relevance_boost += 0.1
        
        confidence += relevance_boost
        
        # Quality indicators
        if 'LIMIT' in sql.upper():
            confidence += 0.05
        if len(sql) > 30 and len(sql) < 300:
            confidence += 0.05
        
        return min(confidence, 1.0)
    
    def _create_intelligent_fallback(self, response: str, question: str) -> Optional[str]:
        """üîÑ Create intelligent SQL fallback"""
        
        question_lower = question.lower()
        
        # Employee-Project relationship
        if any(word in question_lower for word in ['‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô', '‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ', 'assigned']):
            return """SELECT 
                e.name as employee_name,
                p.name as project_name,
                ep.role,
                ep.allocation
            FROM employees e
            JOIN employee_projects ep ON e.id = ep.employee_id
            JOIN projects p ON ep.project_id = p.id
            ORDER BY e.name
            LIMIT 20"""
        
        # Position search
        elif '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á' in question_lower or 'position' in question_lower:
            position = self._extract_position_keyword(question)
            return f"""SELECT name, position, department, salary
            FROM employees
            WHERE position ILIKE '%{position}%'
            ORDER BY name
            LIMIT 20"""
        
        # Department counting
        elif '‡πÅ‡∏ú‡∏ô‡∏Å' in question_lower and any(word in question_lower for word in ['‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô']):
            return """SELECT department, COUNT(*) as employee_count, AVG(salary) as avg_salary
            FROM employees
            GROUP BY department
            ORDER BY employee_count DESC"""
        
        # General employee listing
        else:
            return """SELECT name, position, department, salary
            FROM employees
            ORDER BY name
            LIMIT 20"""
    
    def _extract_position_keyword(self, question: str) -> str:
        """Extract position keyword from question"""
        
        question_lower = question.lower()
        
        position_keywords = ['frontend', 'backend', 'fullstack', 'developer', 'designer', 'manager', 'qa', 'devops']
        
        for keyword in position_keywords:
            if keyword in question_lower:
                return keyword
        
        # Try to extract after "‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á"
        import re
        match = re.search(r'‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á\s*(\w+)', question_lower)
        if match:
            return match.group(1)
        
        return 'developer'  # default
    
    # ========================================================================
    # üóÑÔ∏è DATABASE OPERATIONS (Same as before)
    # ========================================================================
    
    def _get_database_connection(self, tenant_id: str) -> psycopg2.extensions.connection:
        """üîå Get database connection"""
        
        config = self.tenant_configs[tenant_id]
        
        try:
            conn = psycopg2.connect(
                host=config.db_host,
                port=config.db_port,
                database=config.db_name,
                user=config.db_user,
                password=config.db_password,
                connect_timeout=10
            )
            conn.set_session(autocommit=True)
            return conn
            
        except Exception as e:
            logger.error(f"‚ùå Database connection failed for {tenant_id}: {e}")
            raise
    
    async def _execute_sql_unified(self, sql_query: str, tenant_id: str) -> List[Dict[str, Any]]:
        """üóÑÔ∏è UNIFIED: Execute SQL query"""
        
        try:
            logger.info(f"üóÑÔ∏è Executing SQL for {tenant_id}: {sql_query[:100]}...")
            
            conn = self._get_database_connection(tenant_id)
            cursor = conn.cursor()
            
            cursor.execute(sql_query)
            
            # Get results
            columns = [desc[0] for desc in cursor.description] if cursor.description else []
            rows = cursor.fetchall()
            
            # Process results
            results = []
            for row in rows:
                row_dict = dict(zip(columns, row))
                processed_row = self._process_row_data(row_dict)
                results.append(processed_row)
            
            cursor.close()
            conn.close()
            
            logger.info(f"‚úÖ SQL executed successfully: {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå SQL execution failed: {e}")
            logger.error(f"‚ùå Failed SQL: {sql_query}")
            return []
    
    def _process_row_data(self, row_dict: Dict[str, Any]) -> Dict[str, Any]:
        """üîß Process row data"""
        
        processed_row = {}
        
        for key, value in row_dict.items():
            if isinstance(value, Decimal):
                processed_row[key] = float(value)
            elif isinstance(value, (date, datetime)):
                processed_row[key] = value.isoformat()
            elif value is None:
                processed_row[key] = None
            elif isinstance(value, str):
                processed_row[key] = value.strip()
            else:
                processed_row[key] = value
        
        return processed_row
    
    # ========================================================================
    # üîç SCHEMA DISCOVERY (Same as before)
    # ========================================================================
    
    async def _get_schema_unified(self, tenant_id: str) -> Dict[str, Any]:
        """üîç UNIFIED: Get schema info with caching"""
        
        cache_key = f"{tenant_id}_schema"
        
        # Check cache
        if self._is_schema_cache_valid(cache_key):
            logger.info(f"üìä Using cached schema for {tenant_id}")
            return self.schema_cache[cache_key]['data']
        
        try:
            logger.info(f"üîç Discovering schema for {tenant_id}")
            schema_info = await self._discover_schema(tenant_id)
            
            # Cache results
            self.schema_cache[cache_key] = {
                'data': schema_info,
                'timestamp': time.time()
            }
            
            return schema_info
            
        except Exception as e:
            logger.error(f"‚ùå Schema discovery failed: {e}")
            return self._get_fallback_schema()
    
    def _is_schema_cache_valid(self, cache_key: str) -> bool:
        """Check cache validity"""
        if cache_key not in self.schema_cache:
            return False
        
        cache_age = time.time() - self.schema_cache[cache_key]['timestamp']
        return cache_age < self.cache_ttl
    
    async def _discover_schema(self, tenant_id: str) -> Dict[str, Any]:
        """Discover database schema"""
        
        try:
            conn = self._get_database_connection(tenant_id)
            cursor = conn.cursor()
            
            schema_info = {
                'tables': {},
                'sample_data': {},
                'discovered_at': datetime.now().isoformat()
            }
            
            # Get table structure
            cursor.execute("""
                SELECT table_name, column_name, data_type, is_nullable
                FROM information_schema.columns 
                WHERE table_schema = 'public' 
                AND table_name IN ('employees', 'projects', 'employee_projects')
                ORDER BY table_name, ordinal_position
            """)
            
            for row in cursor.fetchall():
                table_name, column_name, data_type, is_nullable = row
                
                if table_name not in schema_info['tables']:
                    schema_info['tables'][table_name] = {'columns': []}
                
                schema_info['tables'][table_name]['columns'].append({
                    'name': column_name,
                    'type': data_type,
                    'nullable': is_nullable == 'YES'
                })
            
            # Get sample data
            for table_name in schema_info['tables'].keys():
                sample_data = self._get_sample_data(cursor, table_name)
                schema_info['sample_data'][table_name] = sample_data
            
            conn.close()
            return schema_info
            
        except Exception as e:
            logger.error(f"Schema discovery error: {e}")
            raise
    
    def _get_sample_data(self, cursor, table_name: str) -> Dict[str, List]:
        """Get sample data for schema"""
        
        sample_data = {}
        
        try:
            important_columns = ['department', 'position', 'name', 'client', 'status']
            
            # Get columns for this table
            cursor.execute(f"""
                SELECT column_name FROM information_schema.columns 
                WHERE table_name = '{table_name}' AND table_schema = 'public'
            """)
            
            available_columns = [row[0] for row in cursor.fetchall()]
            
            for column in important_columns:
                if column in available_columns:
                    cursor.execute(f"""
                        SELECT DISTINCT {column} 
                        FROM {table_name} 
                        WHERE {column} IS NOT NULL 
                        LIMIT 5
                    """)
                    
                    values = [str(row[0]) for row in cursor.fetchall()]
                    sample_data[column] = values
            
            return sample_data
            
        except Exception as e:
            logger.warning(f"Could not get sample data for {table_name}: {e}")
            return {}
    
    def _get_fallback_schema(self) -> Dict[str, Any]:
        """Fallback schema"""
        return {
            'tables': {
                'employees': {
                    'columns': [
                        {'name': 'id', 'type': 'integer', 'nullable': False},
                        {'name': 'name', 'type': 'character varying', 'nullable': False},
                        {'name': 'department', 'type': 'character varying', 'nullable': False},
                        {'name': 'position', 'type': 'character varying', 'nullable': False},
                        {'name': 'salary', 'type': 'numeric', 'nullable': False}
                    ]
                }
            },
            'sample_data': {
                'employees': {
                    'department': ['IT', 'Sales', 'Management'],
                    'position': ['Developer', 'Designer', 'Manager']
                }
            },
            'discovered_at': datetime.now().isoformat(),
            'fallback': True
        }
    
    # ========================================================================
    # üí¨ CONVERSATIONAL PROCESSING (Same as before)
    # ========================================================================
    
    async def _process_conversational_unified(self, question: str, tenant_id: str, intent_result: Dict) -> Dict[str, Any]:
        """üí¨ UNIFIED: Process conversational questions"""
        
        self.stats['conversational_queries'] += 1
        
        config = self.tenant_configs[tenant_id]
        business_emoji = self._get_business_emoji(tenant_id)
        
        if self._is_greeting(question):
            answer = self._create_greeting_response(tenant_id, business_emoji)
        else:
            answer = self._create_general_conversational_response(question, tenant_id, business_emoji)
        
        return {
            "answer": answer,
            "success": True,
            "data_source_used": f"unified_conversational_{config.model_name}",
            "sql_query": None,
            "tenant_id": tenant_id,
            "system_used": "unified_conversational"
        }
    
    async def _process_hybrid_unified(self, question: str, tenant_id: str, intent_result: Dict) -> Dict[str, Any]:
        """üîÑ UNIFIED: Process hybrid questions"""
        
        logger.info(f"üîÑ Using hybrid approach for: {question[:50]}...")
        
        # Try SQL first
        try:
            sql_result = await self._process_sql_unified_with_ai_response(question, tenant_id, intent_result)
            
            if sql_result.get('success') and sql_result.get('db_results_count', 0) > 0:
                sql_result['system_used'] = 'unified_hybrid_sql_ai'
                return sql_result
        except Exception as e:
            logger.warning(f"Hybrid SQL failed: {e}")
        
        # Fallback to conversational
        conv_result = await self._process_conversational_unified(question, tenant_id, intent_result)
        conv_result['system_used'] = 'unified_hybrid_conversational'
        return conv_result
    
    # ========================================================================
    # üîß HELPER METHODS
    # ========================================================================
    
    def _get_business_context_unified(self, tenant_id: str) -> str:
        """üè¢ Get business context"""
        
        contexts = {
            'company-a': """üè¢ ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏Ø - Enterprise Banking & E-commerce
üí∞ ‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏á‡∏¥‡∏ô: ‡∏ö‡∏≤‡∏ó (THB) | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: 800K-3M+ ‡∏ö‡∏≤‡∏ó
üéØ ‡πÄ‡∏ô‡πâ‡∏ô: ‡∏£‡∏∞‡∏ö‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£, CRM, ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
üë• ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å: ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û, ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå, Central Group""",

            'company-b': """üè® ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: ‡∏™‡∏≤‡∏Ç‡∏≤‡∏†‡∏≤‡∏Ñ‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ ‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà - Tourism & Hospitality Technology  
üí∞ ‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏á‡∏¥‡∏ô: ‡∏ö‡∏≤‡∏ó (THB) | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: 300K-800K ‡∏ö‡∏≤‡∏ó
üéØ ‡πÄ‡∏ô‡πâ‡∏ô: ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß, ‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°, ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏•‡πâ‡∏≤‡∏ô‡∏ô‡∏≤
üë• ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å: ‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°‡∏î‡∏∏‡∏™‡∏¥‡∏ï, ‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢""",

            'company-c': """üåç ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: International Office - Global Software Solutions
üí∞ ‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏á‡∏¥‡∏ô: USD ‡πÅ‡∏•‡∏∞ Multi-currency | ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: 1M-4M+ USD  
üéØ ‡πÄ‡∏ô‡πâ‡∏ô: ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡πâ‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®, Global compliance, Multi-currency
üë• ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å: MegaCorp International (USA), Global Finance Corp (Singapore)"""
        }
        
        return contexts.get(tenant_id, contexts['company-a'])
    
    def _get_business_emoji(self, tenant_id: str) -> str:
        emojis = {'company-a': 'üè¶', 'company-b': 'üè®', 'company-c': 'üåç'}
        return emojis.get(tenant_id, 'üíº')
    
    def _is_greeting(self, question: str) -> bool:
        greetings = ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', 'hello', 'hi', '‡∏ä‡πà‡∏ß‡∏¢', 'help', '‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏Ñ‡∏£']
        return any(word in question.lower() for word in greetings)
    
    def _create_greeting_response(self, tenant_id: str, business_emoji: str) -> str:
        config = self.tenant_configs[tenant_id]
        
        greetings = {
            'company-a': f"""‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ AI Assistant ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {config.name} (AI Response v3.1)

{business_emoji} ‡∏£‡∏∞‡∏ö‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ - ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£
ü§ñ NEW: AI-Generated natural language responses
üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
  ‚Ä¢ "‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á frontend ‡∏ö‡πâ‡∏≤‡∏á"
  ‚Ä¢ "‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å IT"  
  ‚Ä¢ "‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"
  ‚Ä¢ "‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏°‡∏µ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£" """,

            'company-b': f"""‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡πÄ‡∏à‡πâ‡∏≤! ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ AI Assistant ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {config.name} (AI Response v3.1)

{business_emoji} ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÅ‡∏•‡∏∞‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏° - ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£
ü§ñ NEW: AI-Generated natural language responses
üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
  ‚Ä¢ "‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á designer ‡∏ö‡πâ‡∏≤‡∏á"
  ‚Ä¢ "‡∏°‡∏µ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"
  ‚Ä¢ "‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÑ‡∏´‡∏ô"
  ‚Ä¢ "‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°‡πÉ‡∏ô‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà" """,

            'company-c': f"""Hello! I'm the AI Assistant for {config.name} (AI Response v3.1)

{business_emoji} International Operations - Ready to help
ü§ñ NEW: AI-Generated natural language responses
üí° Example questions:
  ‚Ä¢ "Who are the frontend developers?"
  ‚Ä¢ "How many employees in each department?"
  ‚Ä¢ "Which projects is each employee working on?"
  ‚Ä¢ "What's the USD budget breakdown?" """
        }
        
        return greetings.get(tenant_id, greetings['company-a'])
    
    def _create_general_conversational_response(self, question: str, tenant_id: str, business_emoji: str) -> str:
        config = self.tenant_configs[tenant_id]
        
        return f"""{business_emoji} AI-Enhanced System ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {config.name}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

ü§ñ NEW: AI-Generated Response System
{'üî• AI Responses: ENABLED' if self.enable_ai_responses else 'üîß AI Responses: Disabled (using hardcode)'}

üí° ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô:
‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô: "‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á [‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á] ‡∏ö‡πâ‡∏≤‡∏á"
‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: "‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å [‡πÅ‡∏ú‡∏ô‡∏Å]"  
‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: "‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"

üöÄ Powered by Unified Agent v3.1 with AI Response Generation"""
    
    # ========================================================================
    # ‚ùå ERROR HANDLING (Same as before)
    # ========================================================================
    
    def _create_error_response(self, error_message: str, tenant_id: str) -> Dict[str, Any]:
        return {
            "answer": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö: {error_message}",
            "success": False,
            "data_source_used": "unified_ai_error",
            "sql_query": None,
            "tenant_id": tenant_id,
            "system_used": "unified_ai_error_handler",
            "error": error_message
        }
    
    def _create_sql_error_response(self, question: str, tenant_id: str, error_message: str) -> Dict[str, Any]:
        config = self.tenant_configs[tenant_id]
        business_emoji = self._get_business_emoji(tenant_id)
        
        answer = f"""{business_emoji} ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‚ö†Ô∏è ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {error_message}

ü§ñ AI Response System: {'Active' if self.enable_ai_responses else 'Disabled'}

üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
‚Ä¢ ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
‚Ä¢ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á frontend ‡∏ö‡πâ‡∏≤‡∏á" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å IT"

‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó"""
        
        return {
            "answer": answer,
            "success": False,
            "data_source_used": f"unified_ai_sql_error_{config.model_name}",
            "sql_query": None,
            "tenant_id": tenant_id,
            "system_used": "unified_ai_sql_error_handler",
            "error_reason": error_message
        }
    
    # ========================================================================
    # üìä ENHANCED STATISTICS WITH AI METRICS
    # ========================================================================
    
    def _update_stats(self, tenant_id: str, success: bool, processing_time: float):
        """Update system statistics"""
        
        if success:
            self.stats['successful_queries'] += 1
        else:
            self.stats['failed_queries'] += 1
        
        # Update average response time
        total_queries = self.stats['total_queries']
        current_avg = self.stats['avg_response_time']
        new_avg = ((current_avg * (total_queries - 1)) + processing_time) / total_queries
        self.stats['avg_response_time'] = new_avg
    
    def get_unified_statistics(self) -> Dict[str, Any]:
        """Get comprehensive statistics with AI metrics"""
        
        success_rate = 0
        if self.stats['total_queries'] > 0:
            success_rate = (self.stats['successful_queries'] / self.stats['total_queries']) * 100
        
        ai_usage_rate = 0
        total_responses = self.stats['ai_responses_used'] + self.stats['hardcode_responses_used']
        if total_responses > 0:
            ai_usage_rate = (self.stats['ai_responses_used'] / total_responses) * 100
        
        return {
            'agent_version': 'unified_v3.1_ai_response',
            'architecture': 'single_file_unified_with_ai_responses',
            'total_queries': self.stats['total_queries'],
            'sql_queries': self.stats['sql_queries'],
            'conversational_queries': self.stats['conversational_queries'],
            'success_rate': round(success_rate, 2),
            'avg_response_time': round(self.stats['avg_response_time'], 3),
            'ai_response_stats': {
                'ai_responses_used': self.stats['ai_responses_used'],
                'hardcode_responses_used': self.stats['hardcode_responses_used'],
                'ai_usage_rate': round(ai_usage_rate, 2),
                'ai_enabled': self.enable_ai_responses,
                'fallback_enabled': self.fallback_to_hardcode,
                'ai_temperature': self.ai_response_temperature
            },
            'features': [
                'unified_sql_generation',
                'complete_sql_extraction', 
                'intelligent_fallback',
                'real_time_schema_discovery',
                'enhanced_intent_detection',
                'ai_generated_responses',  # NEW
                'hardcode_fallback',       # NEW
                'improved_null_handling',  # NEW
                'no_function_duplication'
            ],
            'improvements': [
                'fixed_sql_extraction_bugs',
                'eliminated_function_duplication',
                'simplified_architecture',
                'better_error_handling',
                'comprehensive_validation',
                'ai_natural_language_responses',  # NEW
                'context_aware_formatting',      # NEW
                'business_specific_styling'      # NEW
            ],
            'tenant_configs': list(self.tenant_configs.keys())
        }
    
    # ========================================================================
    # üîÑ COMPATIBILITY METHODS
    # ========================================================================
    
    async def process_question(self, question: str, tenant_id: str) -> Dict[str, Any]:
        """Compatibility method"""
        return await self.process_enhanced_question(question, tenant_id)
    
    async def process_enhanced_question_streaming(self, question: str, tenant_id: str):
        """Streaming version with AI response capability"""
        
        yield {"type": "status", "message": "ü§ñ AI-Enhanced Processing...", "system": "unified_v3.1_ai"}
        
        # Process question
        result = await self.process_enhanced_question(question, tenant_id)
        
        # Stream answer
        answer = result["answer"]
        chunk_size = 100
        
        for i in range(0, len(answer), chunk_size):
            chunk = answer[i:i+chunk_size]
            yield {"type": "answer_chunk", "content": chunk}
        
        # Send completion with AI metrics
        yield {
            "type": "answer_complete",
            "sql_query": result.get("sql_query"),
            "db_results_count": result.get("db_results_count", 0),
            "processing_time_seconds": result.get("processing_time_seconds", 0),
            "tenant_id": tenant_id,
            "system_used": result.get("system_used", "unified_v3.1_ai"),
            "response_generation_method": result.get("response_generation_method", "unknown"),
            "unified_agent_version": "v3.1_ai_response"
        }
    
    # ========================================================================
    # ü©∫ HEALTH CHECK WITH AI METRICS
    # ========================================================================
    
    async def health_check(self) -> Dict[str, Any]:
        """Comprehensive health check with AI response metrics"""
        
        health_status = {
            'overall_status': 'healthy',
            'components': {},
            'issues': [],
            'last_check': datetime.now().isoformat()
        }
        
        # Test Ollama connectivity
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.ollama_base_url}/api/tags",
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    
                    if response.status == 200:
                        health_status['components']['ollama'] = {
                            'status': 'healthy',
                            'url': self.ollama_base_url,
                            'features': ['sql_generation', 'ai_responses']
                        }
                    else:
                        health_status['components']['ollama'] = {
                            'status': 'unhealthy',
                            'error': f"HTTP {response.status}"
                        }
                        health_status['issues'].append("Ollama server not responding properly")
                        
        except Exception as e:
            health_status['components']['ollama'] = {
                'status': 'unhealthy',
                'error': str(e)
            }
            health_status['issues'].append(f"Ollama connectivity failed: {str(e)}")
        
        # Test database connections
        db_status = {}
        for tenant_id in self.tenant_configs.keys():
            try:
                conn = self._get_database_connection(tenant_id)
                cursor = conn.cursor()
                cursor.execute("SELECT 1")
                cursor.close()
                conn.close()
                
                db_status[tenant_id] = 'healthy'
                
            except Exception as e:
                db_status[tenant_id] = f'unhealthy: {str(e)}'
                health_status['issues'].append(f"Database {tenant_id} connection failed")
        
        health_status['components']['databases'] = db_status
        
        # AI Response component status
        health_status['components']['ai_responses'] = {
            'enabled': self.enable_ai_responses,
            'fallback_enabled': self.fallback_to_hardcode,
            'temperature': self.ai_response_temperature,
            'status': 'configured'
        }
        
        # Overall status
        if len(health_status['issues']) > 0:
            health_status['overall_status'] = 'degraded' if len(health_status['issues']) < 3 else 'unhealthy'
        
        # Add system info with AI metrics
        health_status['system_info'] = {
            'agent_version': 'unified_v3.1_ai_response',
            'tenants_configured': len(self.tenant_configs),
            'cache_entries': len(self.schema_cache),
            'total_queries_processed': self.stats['total_queries'],
            'ai_responses_generated': self.stats['ai_responses_used'],
            'hardcode_responses_used': self.stats['hardcode_responses_used']
        }
        
        return health_status


# ============================================================================
# üöÄ USAGE EXAMPLE AND FACTORY
# ============================================================================

def create_unified_ai_agent() -> UnifiedEnhancedPostgresOllamaAgentWithAIResponse:
    """üè≠ Factory function to create unified agent with AI responses"""
    return UnifiedEnhancedPostgresOllamaAgentWithAIResponse()


# Export for compatibility
EnhancedPostgresOllamaAgent = UnifiedEnhancedPostgresOllamaAgentWithAIResponse

# ============================================================================
# üìö DOCUMENTATION
# ============================================================================

"""
ü§ñ Unified Enhanced PostgreSQL + Ollama Agent v3.1 with AI-Generated Responses

üÜï NEW FEATURES:
- AI-Generated natural language responses from database results
- Context-aware response generation based on business type
- Hybrid approach: AI response + hardcode fallback
- Company-specific response styling and tone
- Enhanced NULL/empty data handling in both AI and hardcode responses
- Performance metrics for AI vs hardcode response usage

‚úÖ MAINTAINED FIXES:
- Complete SQL extraction with proper pattern matching
- Eliminated function duplication between files  
- Single source of truth for all SQL operations
- Proper table alias validation
- Intelligent fallback SQL generation
- Enhanced intent detection with confidence scoring
- Real-time schema discovery with caching
- Comprehensive error handling and validation

üèóÔ∏è ARCHITECTURE:
- Single file architecture with AI response capability
- Clear separation: Intent ‚Üí Schema ‚Üí SQL ‚Üí Execute ‚Üí AI Response ‚Üí Format
- Fallback mechanism: AI Response ‚Üí Hardcode Response ‚Üí Error

üéØ AI RESPONSE FEATURES:
1. Natural language generation from database results
2. Business context-aware formatting
3. Company-specific tone and style
4. Automatic truncation for readability
5. Metadata injection for completeness
6. Temperature-controlled creativity
7. Intelligent fallback to hardcode formatting

üîß CONFIGURATION:
Environment variables:
- ENABLE_AI_RESPONSES=true/false (default: true)
- AI_RESPONSE_TEMPERATURE=0.3 (default: 0.3)
- FALLBACK_TO_HARDCODE=true/false (default: true)

üîß USAGE:
```python
agent = UnifiedEnhancedPostgresOllamaAgentWithAIResponse()
result = await agent.process_enhanced_question("‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á frontend ‡∏ö‡πâ‡∏≤‡∏á", "company-a")
print(result['answer'])  # AI-generated natural language response
print(result['response_generation_method'])  # 'ai_generated' or 'hardcode_fallback'
```

üìä BENEFITS:
- More natural, conversational responses
- Better handling of complex data relationships
- Adaptive formatting based on data content
- Improved user experience
- Maintained performance with fallback options
- Complete backward compatibility

üéâ RESULT: Natural language responses that feel human-written while maintaining technical accuracy!
"""