# üî• ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç openwebui_proxy.py ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö streaming ‡∏à‡∏£‡∏¥‡∏á

import os
import json
import asyncio
import aiohttp
from datetime import datetime
from typing import Dict, Any, Optional
import uvicorn
from fastapi import FastAPI, HTTPException, Header, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# CONFIGURATION (‡πÄ‡∏î‡∏¥‡∏°)
# =============================================================================

class ProxyConfig:
    def __init__(self):
        self.n8n_base_url = os.getenv('N8N_BASE_URL', 'http://n8n:5678')
        self.rag_service_url = os.getenv('ENHANCED_RAG_SERVICE', 'http://rag-service:5000')  # üî• ‡πÄ‡∏û‡∏¥‡πà‡∏°
        self.default_tenant = os.getenv('DEFAULT_TENANT', 'company-a')
        self.force_tenant = os.getenv('FORCE_TENANT')
        self.port = int(os.getenv('PORT', '8001'))
        self.tenant_configs = {
            'company-a': {'name': 'SiamTech Bangkok HQ', 'model': 'llama3.1:8b', 'language': 'th'},
            'company-b': {'name': 'SiamTech Chiang Mai Regional', 'model': 'llama3.1:8b', 'language': 'th'},
            'company-c': {'name': 'SiamTech International', 'model': 'llama3.1:8b', 'language': 'en'}
        }

config = ProxyConfig()
app = FastAPI(title=f"OpenWebUI Streaming Proxy v3.0", version="3.0.0")

app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

# =============================================================================
# MODELS (‡πÄ‡∏î‡∏¥‡∏°)
# =============================================================================

class ChatCompletionRequest(BaseModel):
    model: str
    messages: list
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2000
    stream: Optional[bool] = False

# =============================================================================
# üî• STREAMING FUNCTIONS - ‡πÉ‡∏´‡∏°‡πà‡∏´‡∏°‡∏î
# =============================================================================

def get_tenant_id() -> str:
    return config.force_tenant or config.default_tenant

def get_tenant_config(tenant_id: str) -> Dict:
    return config.tenant_configs.get(tenant_id, config.tenant_configs['company-a'])

async def call_rag_service_streaming(tenant_id: str, message: str):
    """üî• ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å RAG service ‡πÅ‡∏ö‡∏ö streaming ‡∏à‡∏£‡∏¥‡∏á‡πÜ"""
    
    payload = {
        "query": message,
        "tenant_id": tenant_id,
        "agent_type": "enhanced_sql",
        "temperature": 0.7,
        "include_insights": True
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{config.rag_service_url}/enhanced-rag-query-stream",
                json=payload,
                headers={"X-Tenant-ID": tenant_id},
                timeout=aiohttp.ClientTimeout(total=120)
            ) as response:
                if response.status == 200:
                    # üî• ‡∏≠‡πà‡∏≤‡∏ô streaming response ‡∏à‡∏≤‡∏Å RAG service
                    async for line in response.content:
                        if line:
                            line_str = line.decode('utf-8').strip()
                            if line_str.startswith('data: '):
                                try:
                                    data_str = line_str[6:]  # ‡∏ï‡∏±‡∏î "data: " ‡∏≠‡∏≠‡∏Å
                                    if data_str and data_str != '[DONE]':
                                        data = json.loads(data_str)
                                        yield data
                                except json.JSONDecodeError:
                                    continue
                else:
                    yield {"type": "error", "message": f"RAG service error: HTTP {response.status}"}
                    
    except Exception as e:
        logger.error(f"RAG service streaming error: {e}")
        yield {"type": "error", "message": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"}

# =============================================================================
# üî• MAIN STREAMING ENDPOINT - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# =============================================================================

@app.post("/v1/chat/completions")
async def chat_completions_streaming(request: ChatCompletionRequest):
    """üî• OpenAI-compatible endpoint with REAL streaming"""
    
    tenant_id = get_tenant_id()
    tenant_config = get_tenant_config(tenant_id)
    
    try:
        if not request.messages:
            raise HTTPException(400, "No messages provided")
        
        # Extract user message
        user_message = ""
        for msg in request.messages:
            if hasattr(msg, 'dict'):
                msg_data = msg.dict()
            elif isinstance(msg, dict):
                msg_data = msg
            else:
                msg_data = {"role": "user", "content": str(msg)}
            
            if msg_data.get("role") == "user":
                user_message = msg_data.get("content", "")
        
        if not user_message:
            raise HTTPException(400, "No user message found")
        
        logger.info(f"üî• Streaming request for {tenant_id}: {user_message[:50]}...")
        
        # üöÄ ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≠ streaming
        if request.stream:
            async def generate_openai_streaming():
                try:
                    # ‡∏™‡πà‡∏á initial chunk
                    initial_chunk = {
                        "id": f"chatcmpl-streaming-{int(datetime.now().timestamp())}",
                        "object": "chat.completion.chunk",
                        "created": int(datetime.now().timestamp()),
                        "model": tenant_config['model'],
                        "choices": [{
                            "index": 0,
                            "delta": {"role": "assistant", "content": ""},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(initial_chunk)}\n\n"
                    
                    # üî• ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å RAG service ‡πÅ‡∏ö‡∏ö streaming
                    answer_started = False
                    full_answer = ""
                    
                    async for chunk in call_rag_service_streaming(tenant_id, user_message):
                        chunk_type = chunk.get("type", "")
                        
                        # üìä ‡πÅ‡∏™‡∏î‡∏á status (optional)
                        if chunk_type == "status":
                            status_chunk = {
                                "id": f"chatcmpl-status-{int(datetime.now().timestamp())}",
                                "object": "chat.completion.chunk",
                                "created": int(datetime.now().timestamp()),
                                "model": tenant_config['model'],
                                "choices": [{
                                    "index": 0,
                                    "delta": {"content": f"‚è≥ {chunk.get('message', '')}\n"},
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(status_chunk)}\n\n"
                        
                        # üî• ‡∏™‡πà‡∏á answer chunks
                        elif chunk_type == "answer_chunk":
                            if not answer_started:
                                # ‡∏™‡πà‡∏á newline ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å status
                                separator_chunk = {
                                    "id": f"chatcmpl-sep-{int(datetime.now().timestamp())}",
                                    "object": "chat.completion.chunk",
                                    "created": int(datetime.now().timestamp()),
                                    "model": tenant_config['model'],
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": "\nüí¨ "},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(separator_chunk)}\n\n"
                                answer_started = True
                            
                            # ‡∏™‡πà‡∏á actual content
                            content = chunk.get("content", "")
                            full_answer += content
                            
                            content_chunk = {
                                "id": f"chatcmpl-content-{int(datetime.now().timestamp())}",
                                "object": "chat.completion.chunk",
                                "created": int(datetime.now().timestamp()),
                                "model": tenant_config['model'],
                                "choices": [{
                                    "index": 0,
                                    "delta": {"content": content},
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(content_chunk)}\n\n"
                        
                        # ‚úÖ ‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß
                        elif chunk_type == "answer_complete":
                            # ‡∏™‡πà‡∏á metadata ‡∏™‡∏£‡∏∏‡∏õ (optional)
                            summary_chunk = {
                                "id": f"chatcmpl-summary-{int(datetime.now().timestamp())}",
                                "object": "chat.completion.chunk",
                                "created": int(datetime.now().timestamp()),
                                "model": tenant_config['model'],
                                "choices": [{
                                    "index": 0,
                                    "delta": {"content": f"\n\nüìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {chunk.get('db_results_count', 0)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | SQL: {chunk.get('sql_generation_method', 'AI')}"},
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(summary_chunk)}\n\n"
                            break
                        
                        # ‚ùå Error
                        elif chunk_type == "error":
                            error_chunk = {
                                "id": f"chatcmpl-error-{int(datetime.now().timestamp())}",
                                "object": "chat.completion.chunk",
                                "created": int(datetime.now().timestamp()),
                                "model": tenant_config['model'],
                                "choices": [{
                                    "index": 0,
                                    "delta": {"content": f"\n‚ùå {chunk.get('message', '‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î')}"},
                                    "finish_reason": "stop"
                                }]
                            }
                            yield f"data: {json.dumps(error_chunk)}\n\n"
                            yield "data: [DONE]\n\n"
                            return
                    
                    # ‡∏™‡πà‡∏á final chunk
                    final_chunk = {
                        "id": f"chatcmpl-final-{int(datetime.now().timestamp())}",
                        "object": "chat.completion.chunk",
                        "created": int(datetime.now().timestamp()),
                        "model": tenant_config['model'],
                        "choices": [{
                            "index": 0,
                            "delta": {},
                            "finish_reason": "stop"
                        }]
                    }
                    yield f"data: {json.dumps(final_chunk)}\n\n"
                    yield "data: [DONE]\n\n"
                    
                except Exception as e:
                    logger.error(f"üî• Streaming error: {e}")
                    error_chunk = {
                        "id": f"chatcmpl-error-{int(datetime.now().timestamp())}",
                        "object": "chat.completion.chunk",
                        "created": int(datetime.now().timestamp()),
                        "model": tenant_config['model'],
                        "choices": [{
                            "index": 0,
                            "delta": {"content": f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö: {str(e)}"},
                            "finish_reason": "stop"
                        }]
                    }
                    yield f"data: {json.dumps(error_chunk)}\n\n"
                    yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                generate_openai_streaming(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "*"
                }
            )
        
        # üîÑ Non-streaming (‡πÄ‡∏î‡∏¥‡∏°) - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å RAG service ‡∏õ‡∏Å‡∏ï‡∏¥
        else:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å RAG service ‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
            async with aiohttp.ClientSession() as session:
                payload = {
                    "query": user_message,
                    "tenant_id": tenant_id,
                    "agent_type": "enhanced_sql"
                }
                
                async with session.post(
                    f"{config.rag_service_url}/enhanced-rag-query",
                    json=payload,
                    headers={"X-Tenant-ID": tenant_id},
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        answer = result.get("answer", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ")
                    else:
                        answer = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (HTTP {response.status})"
            
            return {
                "id": f"chatcmpl-{int(datetime.now().timestamp())}",
                "object": "chat.completion",
                "created": int(datetime.now().timestamp()),
                "model": tenant_config['model'],
                "choices": [{
                    "index": 0,
                    "message": {"role": "assistant", "content": answer},
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": len(user_message.split()),
                    "completion_tokens": len(answer.split()),
                    "total_tokens": len(user_message.split()) + len(answer.split())
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"üî• Unexpected error: {e}")
        return {
            "id": f"chatcmpl-error-{int(datetime.now().timestamp())}",
            "object": "chat.completion",
            "created": int(datetime.now().timestamp()),
            "model": tenant_config['model'],
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö: {str(e)}"},
                "finish_reason": "stop"
            }],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
            "error": str(e)
        }

# =============================================================================
# ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ endpoints ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏î‡∏¥‡∏°
# =============================================================================

@app.get("/health")
async def health():
    tenant_id = get_tenant_id()
    tenant_config = get_tenant_config(tenant_id)
    
    return {
        "status": "healthy",
        "service": "OpenWebUI Streaming Proxy v3.0",
        "version": "3.0.0",
        "tenant_id": tenant_id,
        "tenant_name": tenant_config['name'],
        "model": tenant_config['model'],
        "rag_service": config.rag_service_url,
        "streaming_enabled": True,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/v1/models")
async def list_models():
    tenant_id = get_tenant_id()
    tenant_config = get_tenant_config(tenant_id)
    
    return {
        "object": "list",
        "data": [{
            "id": "llama3.1:8b",
            "object": "model",
            "created": int(datetime.now().timestamp()),
            "owned_by": f"siamtech-{tenant_id}",
            "streaming_supported": True,
            "siamtech_metadata": {
                "tenant_id": tenant_id,
                "tenant_name": tenant_config['name'],
                "language": tenant_config['language'],
                "proxy_version": "3.0.0"
            }
        }]
    }

# =============================================================================
# MAIN APPLICATION
# =============================================================================

if __name__ == "__main__":
    tenant_id = get_tenant_id()
    tenant_config = get_tenant_config(tenant_id)
    
    print("üîó OpenWebUI Streaming Proxy v3.0")
    print("=" * 50)
    print(f"üè¢ Tenant: {tenant_config['name']} ({tenant_id})")
    print(f"ü§ñ Model: {tenant_config['model']}")
    print(f"üåê Language: {tenant_config['language']}")
    print(f"üì° RAG Service: {config.rag_service_url}")
    print(f"üîß Port: {config.port}")
    print(f"üî• Streaming: ENABLED")
    print("=" * 50)
    
    uvicorn.run("openwebui_proxy:app", host="0.0.0.0", port=config.port, reload=False)